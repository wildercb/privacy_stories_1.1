{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8571\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions**\\n\\n1. Collect: Collecting email address and account information for account management, functionality, and authentication.\\n2. Use: Using collected data for account management, functionality, and authentication.\\n3. Share: No sharing of personal data mentioned in the text.\\n\\n**Data Types**\\n\\n1. Contact Data:\\n\\t* Email address\\n\\t* Account information\\n2. Health Data: No health data mentioned in the text.\\n3. Social Media: No social media data mentioned in the text.\\n4. Location: No location data mentioned in the text.\\n5. Financial:\\n\\t* Orders\\n\\t* Payment History\\n\\t* Purchase History\\n\\t* Card Data\\n\\t* Bank Account\\n\\t* Credit Score\\n\\t* Income Information\\n\\t* assets\\n\\t\\t+ vehicle\\n\\t\\t+ Insurance\\n6. Usage Data:\\n\\t* App Interactions:\\n\\t\\t+ Pages Visited\\n\\t\\t+ Timestamps\\n\\t\\t+ Interaction with Ads\\n\\t\\t+ User Engagement\\n\\t\\t+ Session ID\\n\\t* Device Information:\\n\\t\\t+ IP Address\\n\\t\\t+ Device ID\\n\\t\\t+ Advertisement ID\\n\\t\\t+ Browser\\n\\t\\t+ Operating System\\n\\t\\t+ Diagnostics\\n\\t\\t+ Sensor Data\\n\\t\\t+ Audio\\n\\t\\t+ Browsing history\\n7. Tracking:\\n\\t* Cookies\\n\\t* Web Beacons\\n\\t* Tags\\n8. Account Information:\\n\\t* User id\\n\\t* Username\\n\\t* Password\\n\\t* Account Balance\\n\\t* Messages\\n\\t* Friends\\n\\n**Purposes**\\n\\n1. Contact\\n2. Account management\\n3. Functionality\\n4. Authentication\\n5. Security\\n6. Tracking\\n7. Customization\\n8. Accounts\\n9. Requirements\\n\\n**Stories**\\n\\n1. We (collect) email address and account information For account management, we use email address and account information for functionality, and we use email address and account information for authentication.\\n2. We (use) email address and account information for account management.\\n3. We (collect) account information for account management, functionality, and authentication.\\n4. We (use) account information for account management and functionality.\\n5. We (collect) orders, payment history, purchase history, card data, bank account, credit score, income information, and assets for financial purposes.\\n6. We (use) orders, payment history, purchase history, card data, bank account, credit score, income information, and assets for financial purposes.\\n7. We (collect) app interactions, device information, cookies, web beacons, tags, user id, username, password, account balance, messages, and friends for tracking purposes.\\n8. We (use) app interactions, device information, cookies, web beacons, tags, user id, username, password, account balance, messages, and friends for tracking purposes.\\n\\n**Rationale**\\n\\n1. The text describes the process of creating an account on Lemmy, which involves collecting email address and account information for account management, functionality, and authentication.\\n2. The text mentions the use of collected data for account management, functionality, and authentication, and also mentions the purpose of tracking user interactions and financial data.\\n3. The text highlights the importance of security, as it mentions the need to protect user data and prevent spam bots from signing up.\\n4. The text also mentions the customization and customization options available to users, such as choosing a theme and setting up notifications.\\n5. The text provides examples of how user data is used for financial purposes, such as tracking orders, payment history, and purchase history.\\n6. The text also highlights the importance of tracking user interactions and device information, as it mentions the use of cookies, web beacons, and tags to track user behavior.\\n7. The text emphasizes the need for users to be aware of their data and take steps to protect it, as it mentions the need to choose a strong password and enable two-factor authentication.\\n8. The text provides examples of how user data is used for tracking purposes, such as tracking app interactions, device information, cookies, web beacons, tags, user id, username, password, account balance, messages, and friends.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations:\\n\\n**Actions:**\\n\\n1. Collect: \\n   - Collect Email address (Data Type: Contact Data)\\n   - Collect account information (Data Type: Account Information)\\n   - Collect username (Data Type: Contact Data)\\n   - Collect password (Data Type: Account Information)\\n   - Collect displayname (Data Type: Contact Data)\\n   - Collect bio (Data Type: Contact Data)\\n   - Collect account balance (Data Type: Account Information)\\n   - Collect message (Data Type: Account Information)\\n\\n   \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n1. Add Account: \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect, Use, Share \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions**\\n- Collect: Collecting date of birth, email address, image, and friends for contact, customization, and functionality. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect\\n- Collect Date of Birth\\n- Collect Email address\\n- Collect Image\\n- Collect Friends\\n- Collect Contact Information\\nUse\\n- Use Contact Information for Contact\\n- Use Contact Information for Customization\\n- Use Contact Information for Functionality\\n- Use Image for Functionality\\n- Use Image for Customization\\n- Use Image for Contact\\n- Use Friends for Functionality\\n- Use Friends for Customization\\n- Use Friends for Contact\\n\\nData Types:\\n- Date of Birth\\n- Email address\\n- Image\\n- Friends\\n- Contact Information\\n- Contact\\n\\nPurposes:\\n- Contact\\n- Customization\\n- Functionality\\n\\nStories:\\n1. We collect Date of Birth, Email address, Image, and Friends for Contact. We use these data types for Contact. We (Collect) (Date of Birth, Email address, Image, Friends) for (Contact). \\n2. We collect Date of Birth, Email address, Image, and Friends for Customization. We use these data types for Customization. We (Collect) (Date of Birth, Email address, Image, Friends) for (Customization).\\n3. We collect Date of Birth, Email address, Image, and Friends for Functionality. We use these data types for Functionality. We (Collect) (Date of Birth, Email address, Image, Friends) for (Functionality).\\n4. We use Date of Birth, Email address, Image, and Friends for Functionality. We (Use) (Date of Birth, Email address, Image, Friends) for (Functionality).\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts'}\n",
      "  F1 Score: 0.8889\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts'}\n",
      "  F1 Score: 0.8333\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts'}\n",
      "  F1 Score: 0.2857\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\n**Actions**\\n- Collect: Collect email address, phone number, account information, username, password, user ID, job title, company, address, name, date of birth, image, government ID, biographical data (CV, education, employment), health data (physical activity), account data (username, password, email), financial data (orders, payment history, purchase history), usage data (app interactions, device information), tracking data (cookies, web beacons, tags), and account information (user ID, username, password, account balance, messages, friends)\\n- Use: Use collected data to authenticate users, validate third-party identifiers, and manage user accounts\\n- Share: Share user data with third-party services, such as identity servers, and with other Synapse modules\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts', 'contact', 'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts', 'contact', 'functionality', 'account management'}\n",
      "  F1 Score: 0.8333\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'accounts', 'contact', 'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\n**Actions**\\n\\n1. Collect account information\\n2. Collect email address\\n3. Collect phone number\\n4. Use account information\\n5. Use email address\\n6. Use phone number\\n7. Register new user\\n8. Validate email address\\n9. Validate phone number\\n10. Bind email address to account\\n11. Bind phone number to account\\n12. Unbind email address from account\\n13. Unbind phone number from account\\n14. Request token for email address\\n15. Request token for phone number\\n16. Send text message to phone number\\n17. Submit token for email address\\n18. Submit token for phone number\\n19. Delete email address\\n20. Delete phone number\\n\\n**Data Types**\\n\\n1. Account information\\n2. Email address\\n3. Phone number\\n4. User ID\\n5. Job Title\\n6. Company\\n7. Address\\n8. Name\\n9. Date of Birth\\n10. Image\\n11. Government ID\\n12. Biographical Data (CV, Education, Employment)\\n13. Health Data (Physical activity)\\n14. Social Media\\n15. Location (Approximate, Precise)\\n16. Financial (Orders, Payment History, Purchase History, Order, Card Data, Bank Account, Credit Score, Income Information, assets (vehicle, Insurance))\\n17. Usage Data (App Interactions (Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID), Device Information (IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history))\\n18. Tracking (Cookies, Web Beacons, Tags)\\n19. Account Information (User ID, Username, Password, Account Balance, Messages, Friends)\\n\\n**Purposes**\\n\\n1. Contact\\n2. Analytics\\n3. Customization\\n4. Advertisement\\n5. Security\\n6. Tracking\\n7. Functionality\\n8. Accounts\\n9. Requirements\\n\\n**Stories**\\n\\n1. We collect account information, email address and phone number for functionality. We collect account information, email address and phone number for account management. We use account information, email address and phone number for functionality. We use account information, email address and phone number for account management.\\n\\nRationale for annotations:\\n- Actions are identified based on the verbs used in the text, such as \\\"collect\\\", \\\"use\\\", \\\"register\\\", \\\"validate\\\", \\\"bind\\\", \\\"unbind\\\", etc.\\n- Data types are identified based on the specific information being collected, such as \\\"account information\\\", \\\"email address\\\", \\\"phone number\\\", etc.\\n- Purposes are identified based on the intended use of the actions and data types, such as \\\"functionality\\\", \\\"account management\\\", \\\"advertisement\\\", etc.\\n- Stories are identified by grouping actions, data types, and purposes together to describe the interaction between them, such as how account information is collected and used for functionality and account management.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect, Use, Share\\n\\n\\nData Types:\\nContact Data:\\n  Phone Number: Not mentioned in the text.\\n  Email address: Mentioned as \\\"Email address\\\" (R: Email address is a type of contact data that is used for authentication and notifications.)\\n  User ID: Not mentioned in the text.\\n  Job Title: Not mentioned in the text.\\n  Company: Not mentioned in the text.\\n  Address: Not mentioned in the text.\\n  Name: Mentioned as \\\"Display name\\\" and \\\"Nickname\\\" (R: Display name and nickname are types of contact data that can be used to identify users.)\\n  Date of Birth: Not mentioned in the text.\\n  Image: Not mentioned in the text.\\n  Government ID: Not mentioned in the text.\\n  Biographical Data:\\n    CV: Not mentioned in the text.\\n    Education: Not mentioned in the text.\\n    Employment: Not mentioned in the text.\\n\\n\\nHealth Data: Not mentioned in the text.\\n\\n\\nSocial Media: Not mentioned in the text.\\n\\n\\nLocation:\\n  Approximate location: Not mentioned in the text.\\n  Precise location: Not mentioned in the text.\\n\\n\\nFinancial:\\n  Orders: Not mentioned in the text.\\n  Payment History: Not mentioned in the text.\\n  Purchase History: Not mentioned in the text.\\n  Order: Not mentioned in the text.\\n  Card Data: Not mentioned in the text.\\n  Bank Account: Not mentioned in the text.\\n  Credit Score: Not mentioned in the text.\\n  Income Information: Not mentioned in the text.\\n  assets:\\n    vehicle: Not mentioned in the text.\\n    Insurance: Not mentioned in the text.\\n\\n\\nUsage Data:\\n  App Interactions:\\n    Pages Visited: Not mentioned in the text.\\n    Timestamps: Not mentioned in the text.\\n    Interaction with Ads: Not mentioned in the text.\\n    User Engagement: Not mentioned in the text.\\n    Session ID: Not mentioned in the text.\\n\\n\\nDevice Information:\\n  IP Address: Not mentioned in the text.\\n  Device ID: Not mentioned in the text.\\n  Advertisement ID: Not mentioned in the text.\\n  Browser: Not mentioned in the text.\\n  Operating System: Not mentioned in the text.\\n  Diagnostics: Not mentioned in the text.\\n  Sensor Data: Not mentioned in the text.\\n  Audio: Not mentioned in the text.\\n  Browsing history: Not mentioned in the text.\\n\\n\\nTracking:\\n  Cookies: Not mentioned in the text.\\n  Web Beacons: Not mentioned in the text.\\n  Tags: Not mentioned in the text.\\n\\n\\nAccount Information:\\n  User id: Mentioned as \\\"Nickname\\\" and \\\"Login Name\\\" (R: User ID is a type of account information that is used to identify users.)\\n  Username: Mentioned as \\\"Nickname\\\" and \\\"Login Name\\\" (R: Username is a type of account information that is used to identify users.)\\n  Password: Mentioned as \\\"Password\\\" (R: Password is a type of account information that is used for authentication.)\\n  Account Balance: Not mentioned in the text.\\n  Messages: Not mentioned in the text.\\n  Friends: Not mentioned in the text.\\n\\n\\nPurposes:\\nContact, Analytics, Customization, Advertisement, Security, Tracking, Functionality, Accounts, Requirements\\n\\n\\nStories:\\n1. We collect Email Addresses for Contact and Authenticate Purposes (R: Email address is used for authentication and notifications, and for contacting users.)\\n2. We collect Display Name and Nickname for Contact Purposes (R: Display name and nickname are used to identify users, and are used for contacting users.)\\n3. We collect Account Information for Account Management and Security Purposes (R: Account information is used to manage accounts, authenticate users, and ensure security.)\\n4. We collect Device Information for Tracking Purposes (R: Device information is used to track user interactions with the app.)\\n5. We collect Browser and Operating System Information for Customization Purposes (R: Browser and operating system information is used to customize the app's experience.)\\n6. We collect Account Information for Analytics Purposes (R: Account information is used to analyze user behavior and track interactions with the app.)\\n7. We collect Personal Data for Requirement Purposes (R: Personal data is used to comply with requirements and regulations.)\\n8. We use Contact Information for Advertisement Purposes (R: Contact information is used to send targeted advertisements.)\\n9. We use Account Information for Security Purposes (R: Account information is used to secure the app and protect user data.)\\n10. We use Device Information for Functionality Purposes (R: Device information is used to provide a seamless user experience.)\\n11. We use Account Information for Customization Purposes (R: Account information is used to customize the app's experience.)\\n12. We use Personal Data for Requirement Purposes (R: Personal data is used to comply with requirements and regulations.)\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"<Actions: Collect, Use, Share>\\n<Actions: Collect, Use>\\n<Actions: Use, Share>\\n<Data Types: Email address, account information>\\n<Purposes: Account management, Functionality, authenticate>\\n<Stories: We collect Email Address and account information For account management. We use Email Address and account information for authenticate. We use email address and account information for functionality..>\\n\\n<Actions: Collect, Use>\\n<Data Types: Nickname, US-ASCII text characters and numbers>\\n<Purposes: Identify account, Generate web addresses>\\n<Stories: We collect Nickname containing only US-ASCII text characters and numbers, starting with a text character, For identify account. We use Nickname to generate web addresses..>\\n\\n<Actions: Collect, Share>\\n<Data Types: Display Name, Email address, Nickname>\\n<Purposes: Display user information, Establish communications with others>\\n<Stories: We collect and use Display Name as the user's name, Email address for authentication, and Nickname for communication, For display user information. We collect and use Display Name, Email Address and Nickname for establish communications with others..>\\n\\n<Actions: Use, Share>\\n<Data Types: Password>\\n<Purposes: Authenticate, Establish communications with others>\\n<Stories: We use Password to authenticate for login, and share Password with others through nicknames and email addresses, For authenticate and establish communications with others..>\\n\\n<Actions: Use, Share>\\n<Data Types: Personal Data, Online Directory>\\n<Purposes: Establish communications with others, Interact with others in online directory>\\n<Stories: We use and share Personal Data, including email addresses and nicknames, to establish communications with others, and share Personal Data in online directories for interaction, For establish communications with others and interact with others in online directory..>\\n\\n<Actions: Share, Use>\\n<Data Types: Password, Login information>\\n<Purposes: Establish communications with others, Interact with others>\\n<Stories: We share Password through nicknames and email addresses, and use Login information for authentication, For establish communications with others. We use Login information, including password and email address, to interact with others, For interact with others..>\\n\\n<Actions: Use, Share>\\n<Data Types: Settings page, Personal data export file>\\n<Purposes: Customize browsing experience, Export data for relocation>\\n<Stories: We use and share Settings page for customizing browsing experience, and share Personal data export file for relocation, For customize browsing experience. We use and share Personal data export file for relocate Friendica account..>\\n\\n<Actions: Use, Share>\\n<Data Types: Email address, Nickname>\\n<Purposes: Establish communications with others, Interact with others>\\n<Stories: We use and share Email Address and Nickname to establish communications with others, and interact with others, For establish communications with others and interact with others..>\\n\\n<Actions: Use, Share>\\n<Data Types: Account Balance, Messages, Friends>\\n<Purposes: Interact with others, Interact with others>\\n<Stories: We use and share Account Balance, Messages, and Friends to interact with others, For interact with others..>\\n\\n<Actions: Collect, Use>\\n<Data Types: OpenID address, Account information>\\n<Purposes: Authenticate, Establish communications with others>\\n<Stories: We collect and use OpenID address to authenticate, and use Account information for establish communications with others, For authenticate and establish communications with others..>\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"<Actions: Collect, Use, Share>\\n<Actions: Collect, Use, Share>\\n<Actions: Collect, Use, Share>\\n<Actions: Collect, Use, Share>\\n<Actions: Collect, Use, Share>\\n\\n<Data Types: Email address, account information, Username, Password, Displayname, Name, Date of Birth, CV, Education, Employment, Job Title, Company, Address, Phone Number, Government ID, Biographical Data, Physical activity, Social Media, Location, Approximate location, Precise location, Financial information, Orders, Payment History, Purchase History, Financial Data, Card Data, Bank Account, Credit Score, Income Information, assets, vehicle, Insurance, Usage Data: App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history, Tracking: Cookies, Web Beacons, Tags, Account Information: User id, Username, Password, Account Balance, Messages, Friends>\\n\\n<Purposes: Contact, Functionality, Customize, Advertisement, Security, Tracking, Functionality, Accounts, Functionality, Requirements>\\n\\n<Stories>\\n1. We collect Email Address and account information  For account management. We use Email address and account information for Functionality. We use Email address and account information for customization.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users must provide their email address and account information to create an account. The purpose of collecting this information is for account management, functionality, and customization.\\n\\n2. We collect Username and Password  For account management. We use Username and Password for Functionality. We use Username and Password for Security.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users must provide a username and password to create an account. The purpose of collecting this information is for account management, functionality, and security.\\n\\n3. We collect Displayname, Name, Date of Birth, CV, Education, Employment, Job Title, Company, Address, Phone Number, Government ID, Biographical Data  For customization. We use Displayname, Name, Date of Birth, CV, Education, Employment, Job Title, Company, Address, Phone Number, Government ID, Biographical Data  For Functionality.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users must provide various biographical data to create an account. The purpose of collecting this information is for customization and functionality.\\n\\n4. We collect Physical activity, Social Media, Location, Approximate location, Precise location, Financial information, Orders, Payment History, Purchase History, Financial Data, Card Data, Bank Account, Credit Score, Income Information, assets, vehicle, Insurance, Usage Data: App Interactions, Device Information  For tracking.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users may be asked to provide various data types, including physical activity, social media, location, and financial information. The purpose of collecting this information is for tracking and understanding user behavior.\\n\\n5. We collect Cookies, Web Beacons, Tags  For tracking.\\n\\nRationale: The text describes the sign-up process for a social media platform, where cookies, web beacons, and tags may be used to track user behavior and interactions.\\n\\n6. We collect Account Balance, Messages, Friends  For Functionality.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users may have access to account balance, messages, and friends. The purpose of collecting this information is for functionality and user interaction.\\n\\n7. We collect User id, Username, Password, Account Balance, Messages, Friends  For tracking.\\n\\nRationale: The text describes the sign-up process for a social media platform, where users may be tracked through user ID, username, password, account balance, messages, and friends. The purpose of collecting this information is for tracking and understanding user behavior.\\n\\nThese annotations capture the various actions, data types, and purposes described in the text, and provide a concise story of how these elements interact in the context of sign-up processes for social media platforms.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect, Use, Share\\n \\nData Types:\\nContact Data:\\n  Phone Number:\\n  Email address:\\n  User ID:\\n  Job Title:\\n  Company:\\n  Address:\\n  Name:\\n  Date of Birth:\\n  Image:\\n  Government ID:\\n  Biographical Data:\\n    CV:\\n    Education:\\n    Employment:\\nHealth Data:\\n  Physical activity:\\nSocial Media:\\nLocation:\\n  Approximate location:\\n  Precise location:\\nFinancial:\\n  Orders:\\n  Payment History:\\n  Purchase History:\\n  Order:\\n  Card Data:\\n  Bank Account:\\n  Credit Score:\\n  Income Information:\\n  assets:\\n    vehicle:\\n    Insurance:\\nUsage Data:\\n  App Interactions:\\n    Pages Visited:\\n    Timestamps:\\n    Interaction with Ads:\\n    User Engagement:\\n    Session ID:\\n  Device Information:\\n    IP Address:\\n    Device ID:\\n    Advertisement ID:\\n    Browser:\\n    Operating System:\\n    Diagnostics:\\n    Sensor Data:\\n    Audio:\\n    Browsing history:\\nTracking:\\n  Cookies:\\n  Web Beacons:\\n  Tags:\\nAccount Information:\\n  User id:\\n  Username:\\n  Password:\\n  Account Balance:\\n  Messages:\\n  Friends:\\n\\n\\nPurposes:\\nContact, Analytics, Customization, Advertisement, Security, Tracking, Functionality, Accounts, Requirements\\n\\n\\nStories:\\n1. We (Collect) Email address (data type) for (Purpose) Account management. We (Use) Email address (data type) for (Purpose) Functionality. We (Share) Email address (data type) for (Purpose) Tracking.\\n\\n2. We (Collect) Account information (data type) for (Purpose) Account management. We (Use) Account information (data type) for (Purpose) Functionality. We (Share) Account information (data type) for (Purpose) Security.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: {'functionality', 'security', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: {'functionality', 'security', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: {'functionality', 'security', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations with the requested categories:\\n\\n**Actions:**\\n\\n1. Collect -  Email address, account information\\n2. Use -  Password\\n3. Share -  None\\n4. Update -  User information\\n5. Delete -  User account\\n6. Create -  User account\\n7. Ban -  User account\\n8. Edit -  User information\\n9. View -  User account information\\n10. Add -  User account\\n11. Remove -  User account\\n12. Report -  Video to moderators\\n13. Comment -  Video\\n14. Upload -  Video\\n15. Change -  User settings\\n\\n**Data Types:**\\n\\n1. Contact Data:\\n   - Email address\\n   - Phone Number\\n   - User ID\\n   - Job Title\\n   - Company\\n   - Address\\n   - Name\\n   - Date of Birth\\n   - Image\\n   - Government ID\\n   - Biographical Data:\\n    - CV\\n    - Education\\n    - Employment\\n\\n2. Health Data:\\n   - None\\n\\n3. Social Media:\\n   - None\\n\\n4. Location:\\n   - Approximate location\\n   - Precise location\\n\\n5. Financial:\\n   - Orders\\n   - Payment History\\n   - Purchase History\\n   - Order\\n   - Card Data\\n   - Bank Account\\n   - Credit Score\\n   - Income Information\\n   - assets:\\n    - vehicle\\n    - Insurance\\n\\n6. Usage Data:\\n   - App Interactions:\\n    - Pages Visited\\n    - Timestamps\\n    - Interaction with Ads\\n    - User Engagement\\n    - Session ID\\n   - Device Information:\\n    - IP Address\\n    - Device ID\\n    - Advertisement ID\\n    - Browser\\n    - Operating System\\n    - Diagnostics\\n    - Sensor Data\\n    - Audio\\n    - Browsing history\\n\\n7. Tracking:\\n   - Cookies\\n   - Web Beacons\\n   - Tags\\n\\n8. Account Information:\\n   - User id\\n   - Username\\n   - Password\\n   - Account Balance\\n   - Messages\\n   - Friends\\n\\n**Purposes:**\\n\\n1. Contact -  Account management, user protection\\n2. Analytics -  Tracking user behavior\\n3. Customization -  User settings, color scheme\\n4. Advertisement -  Displaying ads\\n5. Security -  Two factor authentication\\n6. Tracking -  Monitoring user interactions\\n7. Functionality -  User interface and backend\\n8. Accounts -  User account management\\n9. Requirements -  User quota management\\n\\n**Stories:**\\n\\n1. We collect email address, account information for account management. We use email address, account information  for user protection.\\n   - Action: Collect\\n   - Data Type: Email address, account information\\n   - Purpose: Account management, user protection\\n\\n2. We update user information for user management. We use user information  for user management.\\n   - Action: Update\\n   - Data Type: User information\\n   - Purpose: User management\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'security', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\n**Actions:**\\nCollect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n1. Edit\\n2. Store\\n3. Send\\n4. Retrieve\\n5. Display\\n6. Deny\\n7. Allow\\n8. Open\\n9. Initialize\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. Send\\n2. Receive\\n3. Clean\\n4. Retrieve\\n5. Display\\n6. Open\\n7. Deny\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions: <Actions>\\n1. Register \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions: \\n1. Collect: Collect email address and account information for account management.\\n\\t\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n1. Collect: Collecting email addresses and account information for account management, authentication, and functionality.\\n2. Use: Using collected data for authentication, account management, and functionality.\\n3. Share: Sharing private messages and comment replies with users.\\n\\n**Data Types:**\\n1. Contact Data:\\n  * Phone Number: Not mentioned\\n  * Email address: Collected and used for authentication, account management, and functionality\\n  * User ID: Not mentioned\\n  * Job Title: Not mentioned\\n  * Company: Not mentioned\\n  * Address: Not mentioned\\n  * Name: Collected and used for authentication\\n  * Date of Birth: Not mentioned\\n  * Image: Used for Gravatar\\n  * Government ID: Not mentioned\\n  * Biographical Data:\\n    * CV: Not mentioned\\n    * Education: Not mentioned\\n    * Employment: Not mentioned\\n2. Health Data: Not mentioned\\n3. Social Media: Not mentioned\\n4. Location:\\n  * Approximate location: Not mentioned\\n  * Precise location: Not mentioned\\n5. Financial:\\n  * Orders: Not mentioned\\n  * Payment History: Not mentioned\\n  * Purchase History: Not mentioned\\n  * Order: Not mentioned\\n  * Card Data: Not mentioned\\n  * Bank Account: Not mentioned\\n  * Credit Score: Not mentioned\\n  * Income Information: Not mentioned\\n  * assets:\\n    * vehicle: Not mentioned\\n    * Insurance: Not mentioned\\n6. Usage Data:\\n  * App Interactions:\\n    * Pages Visited: Not mentioned\\n    * Timestamps: Not mentioned\\n    * Interaction with Ads: Not mentioned\\n    * User Engagement: Not mentioned\\n    * Session ID: Not mentioned\\n  * Device Information:\\n    * IP Address: Not mentioned\\n    * Device ID: Not mentioned\\n    * Advertisement ID: Not mentioned\\n    * Browser: Not mentioned\\n    * Operating System: Not mentioned\\n    * Diagnostics: Not mentioned\\n    * Sensor Data: Not mentioned\\n    * Audio: Not mentioned\\n    * Browsing history: Not mentioned\\n7. Tracking:\\n  * Cookies: Not mentioned\\n  * Web Beacons: Not mentioned\\n  * Tags: Not mentioned\\n8. Account Information:\\n  * User id: Used for authentication\\n  * Username: Used for authentication and account management\\n  * Password: Used for authentication\\n  * Account Balance: Not mentioned\\n  * Messages: Shared with users\\n  * Friends: Not mentioned\\n\\n**Purposes:**\\n1. Contact\\n2. Analytics\\n3. Customization\\n4. Advertisement\\n5. Security\\n6. Tracking\\n7. Functionality\\n8. Accounts\\n9. Requirements\\n\\n**Stories:**\\n1. We collect Email Addresses and account information For account management. We use Email address and account information for authenticate. We use email address and account information for functionality.\\n2. We share Messages with users For communication.\\n\\n**Rationale:**\\nThe annotations are based on the actions and data types mentioned in the text. For example, \\\"Collect\\\" and \\\"Use\\\" are used to describe the actions performed on email addresses and account information. \\\"Data Types\\\" are categorized according to the types of data mentioned in the text, such as contact data, location, and usage data. \\\"Purposes\\\" are categorized according to the intentions behind the actions and data types, such as contact, analytics, and customization. Finally, \\\"Stories\\\" are created by combining the actions, data types, and purposes to describe the context in which they interact.\\n\\nThe stories highlight that email addresses and account information are collected for account management and authentication, and that messages are shared with users for communication. These stories provide a concise and informative summary of the actions and data types mentioned in the text, and help to illustrate the purposes behind them.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization']\n",
      "  Matched Labels: {'functionality', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n\\n1. Collect: Collecting email addresses and account information for account management, authentication, and functionality.\\n\\nRationale: The text states that \\\"We collect Email Addresses and account information  For account management.\\\" This implies that the email addresses and account information are being collected for the purpose of managing the user's account.\\n\\n2. Use: Using email addresses and account information for authentication and functionality.\\n\\nRationale: The text states that \\\"We use Email address and account information for authenticate. We use email address and account information for functionality.\\\" This implies that the email addresses and account information are being used to authenticate the user and enable functionality.\\n\\n3. Share: Not explicitly stated but implied through registration and account management.\\n\\nRationale: While not explicitly stated, the text implies that users are sharing their personal information through registration and account management processes.\\n\\n**Data Types:**\\n\\n1. Contact Data: Email addresses and account information.\\n\\nRationale: The text specifically mentions collecting email addresses and account information, which are both examples of contact data.\\n\\n2. Health Data: Not explicitly mentioned but implied through the use of username and displayname.\\n\\nRationale: While not explicitly mentioned, the use of username and displayname could be considered health data, as it provides information about the user's identity.\\n\\n3. Social Media: Not explicitly mentioned but implied through the use of username and displayname.\\n\\nRationale: The use of username and displayname could be considered social media, as it provides information about the user's online identity.\\n\\n4. Health Biographical Data: CV, Education, Employment, and Date of Birth.\\n\\nRationale: The text mentions that \\\"Instance admins can set an arbitrary question which needs to be answered in order to create an account. This is often used to prevent spam bots from signing up.\\\" This implies that users may be providing biographical data, including CV, education, employment, and date of birth.\\n\\n5. Usage Data: App Interactions, Device Information.\\n\\nRationale: The text mentions that users can interact with the application and provide device information, which is an example of usage data.\\n\\n**Purposes:**\\n\\n1. Contact: Account management and authentication.\\n\\nRationale: The text states that \\\"We collect Email Addresses and account information  For account management.\\\" This implies that the purpose of collecting email addresses and account information is for account management and authentication.\\n\\n2. Customization: Displayname and Avatar.\\n\\nRationale: The text mentions that users can update their displayname and avatar, which implies that the purpose of these features is customization.\\n\\n3. Advertisement: Not explicitly stated but implied through the use of ads.\\n\\nRationale: While not explicitly stated, the text implies that the application may use ads, which is a form of advertisement.\\n\\n4. Security: Verification of email addresses and account information.\\n\\nRationale: The text states that \\\"Providing an email address is usually optional, but admins may choose to make it mandatory. In this case you will have to wait for a confirmation mail and click the link after completing this form.\\\" This implies that the purpose of verifying email addresses and account information is for security.\\n\\n5. Functionality: Authentication and account management.\\n\\nRationale: The text states that \\\"We use Email address and account information for authenticate. We use email address and account information for functionality.\\\" This implies that the purpose of collecting email addresses and account information is for authentication and enabling functionality.\\n\\n**Stories:**\\n\\n1. We (Collect) Email address (Data Type) for (Contact) Account management (Purpose) We use Email address (Data Type) for (Contact) Authenticate (Purpose)\\n\\nRationale: This story illustrates the collection of email addresses for account management and authentication purposes.\\n\\n2. We (Use) Email address (Data Type) for (Contact) Functionality (Purpose) We use Email address (Data Type) for (Contact) Authenticate (Purpose)\\n\\nRationale: This story illustrates the use of email addresses for functionality and authentication purposes.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here is the annotated text with actions, data types, purposes, and stories:\\n\\nActions:\\n1. Edit \\n2. Lock down \\n3. Send \\n4. Open \\n5. Remove \\n6. Retrieve \\n7. Display \\n8. Handle \\n9. Allow \\n10. Deny \\n11. Check \\n12. Enable \\n\\nData Types:\\n1. Contact Data:\\n    1. Phone Number: \\n    2. Email address: \\n    3. User ID: \\n    4. Job Title: \\n    5. Company: \\n    6. Address: \\n    7. Name: \\n    8. Date of Birth: \\n    9. Image: \\n    10. Government ID: \\n    11. Biographical Data:\\n        1. CV: \\n        2. Education: \\n        3. Employment: \\n2. Health Data:\\n    1. Physical activity: \\n3. Social Media:\\n    1. Links: \\n    2. User Engagement: \\n4. Location:\\n    1. Approximate location: \\n    2. Precise location: \\n5. Financial:\\n    1. Orders: \\n    2. Payment History: \\n    3. Purchase History: \\n    4. Order: \\n    5. Card Data:\\n        1. Bank Account: \\n        2. Credit Score: \\n        3. Income Information:\\n            1. Assets:\\n                1. vehicle: \\n                2. Insurance: \\n6. Usage Data:\\n    1. App Interactions:\\n        1. Pages Visited:\\n        2. Timestamps:\\n        3. Interaction with Ads:\\n        4. User Engagement:\\n        5. Session ID:\\n    2. Device Information:\\n        1. IP Address:\\n        2. Device ID:\\n        3. Advertisement ID:\\n        4. Browser:\\n        5. Operating System:\\n        6. Diagnostics:\\n        7. Sensor Data:\\n        8. Audio:\\n        9. Browsing history:\\n7. Tracking:\\n    1. Cookies:\\n    2. Web Beacons:\\n    3. Tags:\\n8. Account Information:\\n    1. User id:\\n    2. Username:\\n    3. Password:\\n    4. Account Balance:\\n    5. Messages:\\n    6. Friends:\\n\\nPurposes:\\n1. Security: \\n2. Functionality: \\n3. Contact: \\n4. Analytics: \\n5. Customization: \\n6. Advertisement: \\n7. Tracking: \\n8. Requirements: \\n\\nStories:\\n1. We use Contact Data for Contact. \\n2. We use Usage Data for Functionality. \\n3. We use Health Data for Health. \\n4. We use Social Media for Social Media. \\n5. We use Location for Location. \\n6. We use Financial for Financial. \\n7. We use Usage Data for Functionality. \\n8. We use Tracking for Tracking. \\n9. We use Account Information for Account Information. \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['requirements', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'requirements'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. Edit on GitHub R The actions performed in this text include editing a text file and navigating through a web application.\\n2. Lock down APIs R The actions involve exposing certain Electron (and NodeJS) APIs to the front-end application and locking down the exposed APIs.\\n3. Navigate between tabs R The actions involve navigating between distinct tabs bound to the same server in the Desktop App.\\n4. Send messages R The actions involve sending messages to other sources using the phiMail Direct messaging server.\\n5. Receive messages R The actions involve receiving messages from other sources using the phiMail Direct messaging server.\\n6. Handle events R The actions involve handling events such as will-navigate, did-start-navigation, and new-window.\\n7. Open new windows R The actions involve opening new windows or tabs, and denying or allowing the opening of new windows.\\n\\nData Types:\\n1. Phone Number R Not mentioned in the text.\\n2. Email address R Not mentioned in the text.\\n3. User ID R Not mentioned in the text.\\n4. Job Title R Not mentioned in the text.\\n5. Company R Not mentioned in the text.\\n6. Address R Not mentioned in the text.\\n7. Name R Not mentioned in the text.\\n8. Date of Birth R Not mentioned in the text.\\n9. Image R Not mentioned in the text.\\n10. Government ID R Not mentioned in the text.\\n11. Biographical Data:\\n  11.1 CV R Not mentioned in the text.\\n  11.2 Education R Not mentioned in the text.\\n  11.3 Employment R Not mentioned in the text.\\n12. Health Data:\\n  12.1 Physical activity R Not mentioned in the text.\\n13. Social Media R Not mentioned in the text.\\n14. Location:\\n  14.1 Approximate location R Not mentioned in the text.\\n  14.2 Precise location R Not mentioned in the text.\\n15. Financial:\\n  15.1 Orders R Not mentioned in the text.\\n  15.2 Payment History R Not mentioned in the text.\\n  15.3 Purchase History R Not mentioned in the text.\\n  15.4 Order R Not mentioned in the text.\\n  15.5 Card Data R Not mentioned in the text.\\n  15.6 Bank Account R Not mentioned in the text.\\n  15.7 Credit Score R Not mentioned in the text.\\n  15.8 Income Information R Not mentioned in the text.\\n  15.9 assets:\\n  15.9.1 vehicle R Not mentioned in the text.\\n  15.9.2 Insurance R Not mentioned in the text.\\n16. Usage Data:\\n  16.1 App Interactions:\\n  16.1.1 Pages Visited R Not mentioned in the text.\\n  16.1.2 Timestamps R Not mentioned in the text.\\n  16.1.3 Interaction with Ads R Not mentioned in the text.\\n  16.1.4 User Engagement R Not mentioned in the text.\\n  16.1.5 Session ID R Not mentioned in the text.\\n  16.2 Device Information:\\n  16.2.1 IP Address R Not mentioned in the text.\\n  16.2.2 Device ID R Not mentioned in the text.\\n  16.2.3 Advertisement ID R Not mentioned in the text.\\n  16.2.4 Browser R Not mentioned in the text.\\n  16.2.5 Operating System R Not mentioned in the text.\\n  16.2.6 Diagnostics R Not mentioned in the text.\\n  16.2.7 Sensor Data R Not mentioned in the text.\\n  16.2.8 Audio R Not mentioned in the text.\\n  16.2.9 Browsing history R Not mentioned in the text.\\n17. Tracking:\\n  17.1 Cookies R Not mentioned in the text.\\n  17.2 Web Beacons R Not mentioned in the text.\\n  17.3 Tags R Not mentioned in the text.\\n18. Account Information:\\n  18.1 User id R Not mentioned in the text.\\n  18.2 Username R Not mentioned in the text.\\n  18.3 Password R Not mentioned in the text.\\n  18.4 Account Balance R Not mentioned in the text.\\n  18.5 Messages R Not mentioned in the text.\\n  18.6 Friends R Not mentioned in the text.\\n\\nPurposes:\\n1. Security R The purpose of locking down APIs and denying or allowing the opening of new windows is for security purposes.\\n2. Functionality R The purpose of sending and receiving messages, navigating between tabs, and handling events is for functionality purposes.\\n3. Customization R Not mentioned in the text.\\n4. Advertisement R Not mentioned in the text.\\n5. Tracking R The purpose of handling events and opening new windows is for tracking purposes.\\n6. Accounts R The purpose of managing user accounts and messages is for accounts purposes.\\n7. Requirements R Not mentioned in the text.\\n\\nStories:\\n1. We use usage data for Functionality. We use usage data for Security. R The story involves the use of usage data for functionality and security purposes in the context of sending and receiving messages, navigating between tabs, and handling events.\\n2. We use data types for Contact, Analytics, Customization, Advertisement, Security, Tracking, Functionality, Accounts, Requirements. R The story involves the use of data types for various purposes, including contact, analytics, customization, advertisement, security, tracking, functionality, accounts, and requirements.\\n3. We use app interactions for Functionality, Security, Tracking. R The story involves the use of app interactions for functionality, security, and tracking purposes.\\n4. We use device information for Functionality, Security, Tracking. R The story involves the use of device information for functionality, security, and tracking purposes.\\n5. We use tracking for Security, Functionality, Accounts. R The story involves the use of tracking for security, functionality, and accounts purposes.\\n6. We use account information for Security, Functionality, Accounts. R The story involves the use of account information for security, functionality, and accounts purposes.\\n7. We use location for Functionality, Security. R The story involves the use of location for functionality and security purposes.\\n8. We use health data for Functionality, Security. R The story involves the use of health data for functionality and security purposes.\\n9. We use social media for Functionality, Security. R The story involves the use of social media for functionality and security purposes.\\n10. We use financial data for Functionality, Security. R The story involves the use of financial data for functionality and security purposes.\\n11. We use usage data for Functionality. R The story involves the use of usage data for functionality purposes.\\n12. We use tracking for Functionality, Security. R The story involves the use of tracking for functionality and security purposes.\\n13. We use account information for Functionality, Security. R The story involves the use of account information for functionality and security purposes.\\n14. We use location for Functionality, Security. R The story involves the use of location for functionality and security purposes.\\n15. We use health data for Functionality, Security. R The story involves the use of health data for\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the text:\\n\\n**Actions**\\n\\n1. Direct Messaging with LibreEHR and EMR Direct phiMail(R) \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotated text with actions, data types, purposes, and stories:\\n\\nActions:\\n1. Direct Messaging: Share, Collect, Use\\n2. Test Mode: Collect, Use\\n3. Production Mode: Share, Collect, Use\\n4. Enable Direct Messaging: Collect, Use, Share\\n5. Receive Direct Message: Use, Share\\n6. Send Direct Message: Share, Use\\n7. Enable or Disable Direct Messaging: Collect, Use\\n8. Receive Notification Email: Use, Collect\\n\\nData Types:\\n1. Health Data: Physical activity, Biographical Data (CV, Education, Employment)\\n2. Address: Contact Data (Phone Number, Email address, User ID, Job Title, Company, Date of Birth)\\n3. Usage Data: App Interactions (Pages Visited, Timestamps, Interaction with Ads, User Engagement)\\n\\nPurposes:\\n1. Requirements: Collect, Use, Share Health Data, Address\\n2. Functionality: Collect, Use, Share Health Data, Address, Usage Data\\n3. Security: Use, Collect Health Data, Address\\n4. Accounts: Collect, Use User ID, Username, Password, Account Balance\\n\\nStories:\\n1. We (Enable Direct Messaging) (Collect, Use, Share) (Purpose: Functionality)\\n2. We (Share) (Health data) (Purpose: Requirements)\\n3. We (Use) (Health data, Address, Usage data) (Purpose: Functionality)\\n4. We (Collect) (Address) (Purpose: Requirements)\\n5. We (Use) (Address) (Purpose: Functionality)\\n6. We (Collect) (Health data) (Purpose: Functionality)\\n7. We (Use) (Health data, Address, Usage data) (Purpose: Functionality)\\n8. We (Share) (Usage data) (Purpose: Functionality)\\n\\nRationale:\\nThe annotated text is about Direct Messaging in LibreEHR, which is a system for sharing and receiving protected health information. The actions include enabling and disabling Direct Messaging, sending and receiving direct messages, and receiving notification emails. The data types include health data, address, and usage data. The purposes include requirements, functionality, security, and accounts. The stories describe how these actions, data types, and purposes interact to enable the functionality of Direct Messaging in LibreEHR.\\n\\nThe annotations are based on the structure and content of the text, which includes information about the purpose of Direct Messaging, the types of data involved, and the actions required to use it. The stories are concise summaries of how these elements interact to achieve the desired functionality.\\n\\nThe annotation of purposes as requirements and functionality is based on the fact that the text describes how Direct Messaging is used to meet the requirements of the Direct Project and to provide a functional interface for users. The annotation of purposes as security is based on the fact that the text describes the measures taken to secure the Direct Messaging system.\\n\\nThe annotation of stories is based on the fact that the text describes the actions, data types, and purposes in a way that is consistent with a narrative structure. The stories are summaries of how these elements interact to achieve the desired functionality.\\n\\nOverall, the annotations provide a clear and concise summary of the text, highlighting the key elements and how they interact to enable the functionality of Direct Messaging in LibreEHR.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. Collect\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\n**Actions:**\\n1. use \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'requirements', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\nActions:\\n<Actions>use, handle, deny, allow, open, update, create, send, retrieve, display, redirect, access</Actions>\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n\\n1. Collect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['health analysis', 'customization']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Annotations:**\\n\\n1. Actions:\\n   Collect: id, sid, authenticated, distributor, format, priority, adverb, addr, sid, authenticated, class, realm, target\\n   Use: id, sid, authenticated, distributor, format, priority, adverb, addr, sid, authenticated, class\\n   \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotated text with actions, data types, purposes, and stories:\\n\\nActions:\\n- Collect: Collecting Email address and account information \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n- Collect\\n- Use\\n- Share\\n- Create\\n- Edit\\n- Delete\\n- Review\\n- Verify\\n- Play\\n\\nData Types:\\n- Contact Data:\\n  - Email address\\n  - Username\\n  - Displayname\\n  - Email\\n  - Password\\n- Health Data:\\n  - Physical activity\\n  - Biographical Data:\\n    - CV\\n    - Education\\n    - Employment\\n- Social Media:\\n  - Posts\\n  - Comments\\n  - Community\\n- Location:\\n  - Approximate location\\n  - Precise location\\n- Financial:\\n  - Payment History\\n  - Order\\n  - Card Data\\n  - Bank Account\\n  - Credit Score\\n- Usage Data:\\n  - App Interactions:\\n    - Pages Visited\\n    - Timestamps\\n    - Interaction with Ads\\n    - User Engagement\\n    - Session ID\\n  - Device Information:\\n    - IP Address\\n    - Device ID\\n    - Advertisement ID\\n    - Browser\\n    - Operating System\\n    - Diagnostics\\n    - Sensor Data\\n    - Audio\\n- Tracking:\\n  - Cookies\\n  - Web Beacons\\n  - Tags\\n- Account Information:\\n  - User id\\n  - Username\\n  - Password\\n  - Account Balance\\n  - Messages\\n  - Friends\\n\\nPurposes:\\n- Contact\\n- Analytics\\n- Customization\\n- Advertisement\\n- Security\\n- Tracking\\n- Functionality\\n- Accounts\\n- Requirements\\n\\nStories:\\n1. We Collect Email address and account information for Account management. We use Email address and account information for Functionality and Authenticate.\\nManage Users & Auth\\n\\n2. We Create users manually using the \\\"create user\\\" button at the top right of the user menu for User management.\\nManage users\\n\\n3. We Collect and verify registration information such as email address and password for Account management.\\nRegistration\\n\\n4. We Review registration requests manually by administrators or moderators for Registration approval.\\nRegistration approval\\n\\n5. We Collect and update user information such as email address and role for User management.\\nUser management\\n\\n6. We Create, Edit, and Delete videos to user accounts associated with one of their channels for User management.\\nUser management\\n\\n7. We Collect and review video abuse reports for Moderation.\\nModeration\\n\\n8. We Use analytics to track user interactions such as pages visited and timestamps for Analytics.\\nAnalytics\\n\\n9. We Collect and manage user settings such as displayname and bio for Customization.\\nCustomization\\n\\n10. We Collect and track user engagement such as user engagement and interaction with ads for Advertisement.\\nAdvertisement\\n\\n11. We Collect and manage user account information such as username and password for Accounts.\\nAccounts\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n\\n1. Collect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. Manage Users & Auth\\n2. Registration\\n3. Require email verification\\n4. Require approval\\n5. Register\\n6. Create user\\n7. Update user\\n8. Edit user\\n9. Delete user\\n10. Ban\\n11. Add accounts\\n12. View user account information\\n13. Record event\\n14. Send event\\n15. Setup analytics\\n16. Log event\\n17. Identify\\n18. Logout\\n19. Generate device id\\n20. Track event\\n\\nData Types:\\n1. Contact Data:\\n\\t* Email address\\n\\t* Phone number\\n\\t* Job Title\\n\\t* Company\\n\\t* Address\\n\\t* Name\\n\\t* Date of Birth\\n\\t* Image\\n\\t* Government ID\\n\\t* Biographical Data:\\n\\t\\t+ CV\\n\\t\\t+ Education\\n\\t\\t+ Employment\\n2. Health Data:\\n\\t* Physical activity\\n3. Social Media:\\n\\t* Approximate location\\n\\t* Precise location\\n\\t* Order\\n\\t* Payment History\\n\\t* Purchase History\\n\\t* Card Data:\\n\\t\\t+ Bank Account\\n\\t\\t+ Credit Score\\n\\t\\t+ Income Information\\n\\t\\t+ assets:\\n\\t\\t\\t- vehicle\\n\\t\\t\\t- Insurance\\n4. Usage Data:\\n\\t* App Interactions:\\n\\t\\t+ Pages Visited\\n\\t\\t+ Timestamps\\n\\t\\t+ Interaction with Ads\\n\\t\\t+ User Engagement\\n\\t\\t+ Session ID\\n\\t* Device Information:\\n\\t\\t+ IP Address\\n\\t\\t+ Device ID\\n\\t\\t+ Advertisement ID\\n\\t\\t+ Browser\\n\\t\\t+ Operating System\\n\\t\\t+ Diagnostics\\n\\t\\t+ Sensor Data\\n\\t\\t+ Audio\\n\\t\\t+ Browsing history\\n5. Tracking:\\n\\t* Cookies\\n\\t* Web Beacons\\n\\t* Tags\\n6. Account Information:\\n\\t* User id\\n\\t* Username\\n\\t* Password\\n\\t* Account Balance\\n\\t* Messages\\n\\t* Friends\\n\\nPurposes:\\n1. Contact\\n2. Analytics\\n3. Customization\\n4. Advertisement\\n5. Security\\n6. Tracking\\n7. Functionality\\n8. Accounts\\n9. Requirements\\n\\nStories:\\n1. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to manage user accounts.\\n2. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n3. We (Register) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to register users.\\n4. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Account management) to manage user accounts.\\n5. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: User protection) to protect user accounts.\\n6. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Account management) to manage user accounts.\\n7. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n8. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to send notifications to users.\\n9. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Account management) to manage user accounts.\\n10. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: User protection) to protect user accounts.\\n11. We (Register) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to send registration confirmation to users.\\n12. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n13. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Account management) to manage user accounts.\\n14. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to send user account information to administrators.\\n15. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n16. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n17. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Account management) to manage user accounts.\\n18. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to contact users.\\n19. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: Contact) to send notifications to users.\\n20. We (Manage Users & Auth) Collect (Contact Data: Email address, account information) for (Purpose: User protection) to protect user accounts.\\n\\nRationale: The annotations are based on the context of the text, which describes a system for managing user accounts and registering new users. The actions, data types, and purposes are identified based on the specific functionality of the system, such as collecting contact information, managing user accounts, and tracking user behavior. The stories are built from the annotated actions, data types, and purposes to describe how the system interacts with users and collects and uses their information.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect, use\\nData Types: account data, email address, phone number\\nPurposes: functionality, account management\\nStories:\\n1. We collect account data, email address and phone number for functionality. We use account data, email address and phone number for functionality. We use account data, email address and phone number for account management. We collect account data, email address and phone number for account management. We use account data, email address and phone number for account management.\\n\\nRationale: The text states that the `register_account_data_callbacks` method is used to react to changes of account data, and that account data callbacks are available for three types: `on_account_data_updated`, `on_account_data_created`, and `on_account_data_deleted`. The callback `on_account_data_updated` is used to log new account data, which involves collecting and using account data, email address, and phone number for functionality.\\n\\n# Account data callbacks\\n\\nThe available account data callbacks are:\\n\\nActions:\\nCollect, use\\nData Types: account data, room ID, account data type, content\\nPurposes: functionality, account management\\nStories:\\n1. We collect account data, room ID, account data type, and content for functionality. We use account data, room ID, account data type, and content for functionality. We use account data, room ID, account data type, and content for account management. We collect account data, room ID, account data type, and content for account management. We use account data, room ID, account data type, and content for account management.\\n\\nRationale: The text states that the `on_account_data_updated` callback is triggered when new account data is added or when the data associated with a given type (and optionally room) changes. This callback is used to log new account data, which involves collecting and using account data, room ID, account data type, and content for functionality.\\n\\n## `on_account_data_updated`\\n\\nActions:\\nCollect, use\\nData Types: account data, room ID, account data type, content\\nPurposes: functionality, account management\\nStories:\\n1. We collect account data, room ID, account data type, and content for functionality. We use account data, room ID, account data type, and content for functionality. We use account data, room ID, account data type, and content for account management. We collect account data, room ID, account data type, and content for account management. We use account data, room ID, account data type, and content for account management.\\n\\nRationale: The text states that the `on_account_data_updated` callback is triggered when new account data is added or when the data associated with a given type (and optionally room) changes. This callback is used to log new account data, which involves collecting and using account data, room ID, account data type, and content for functionality.\\n\\n## Example\\n\\nThe example below is a module that implements the `on_account_data_updated` callback, and sends an event to an audit room when a user changes their account data.\\n\\nActions:\\nCollect, use\\nData Types: audit room ID, sender ID, event content\\nPurposes: functionality\\nStories:\\n1. We collect audit room ID, sender ID, and event content for functionality. We use audit room ID, sender ID, and event content for functionality. We use audit room ID, sender ID, and event content for functionality.\\n\\nRationale: The text states that the `create_and_send_event_into_room` method is used to send an event to an audit room, which involves collecting and using audit room ID, sender ID, and event content for functionality.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here is the annotated text:\\n\\nActions: Collect, Use\\nData Types: Account data, Email address, Phone number\\nPurposes: Functionality, Account management\\nStories:\\n1. We (register) (account data) (functionality) We collect account data, email address and phone number for functionality. We use account data, email address and phone number for functionality.\\n2. We (register) (account data) (account management) We collect account data, email address and phone number for account management. We use account data, email address and phone number for account management.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n1. Collect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. Collect\\n2. Use\\n3. Share\\n\\nData Types:\\n1. Contact Data:\\n   - Phone Number\\n   - Email address\\n   - User ID\\n   - Job Title\\n   - Company\\n   - Address\\n   - Name\\n   - Date of Birth\\n   - Image\\n   - Government ID\\n   - Biographical Data:\\n      - CV\\n      - Education\\n      - Employment\\n2. Health Data:\\n   - Physical activity\\n3. Social Media:\\n   - None mentioned\\n4. Location:\\n   - Approximate location\\n   - Precise location\\n5. Financial:\\n   - Orders\\n   - Payment History\\n   - Purchase History\\n   - Order\\n   - Card Data\\n   - Bank Account\\n   - Credit Score\\n   - Income Information\\n   - assets:\\n      - vehicle\\n      - Insurance\\n6. Usage Data:\\n   - App Interactions:\\n      - Pages Visited\\n      - Timestamps\\n      - Interaction with Ads\\n      - User Engagement\\n      - Session ID\\n   - Device Information:\\n      - IP Address\\n      - Device ID\\n      - Advertisement ID\\n      - Browser\\n      - Operating System\\n      - Diagnostics\\n      - Sensor Data\\n      - Audio\\n      - Browsing history\\n7. Tracking:\\n   - Cookies\\n   - Web Beacons\\n   - Tags\\n8. Account Information:\\n   - User id\\n   - Username\\n   - Password\\n   - Account Balance\\n   - Messages\\n   - Friends\\n\\nPurposes:\\n1. Contact\\n2. Analytics\\n3. Customization\\n4. Advertisement\\n5. Security\\n6. Tracking\\n7. Functionality\\n8. Accounts\\n9. Requirements\\n\\nStories:\\n1. We use account information for security.\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\n1. collect \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\nCollect, Use, Share \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:** \\n1. Collect messages, username \\n2. Use messages, username \\n3. Collect \\n4. Use \\n5. Collect username \\n6. Use username \\n7. Collect notify_subscription, notify_watch, id \\n8. Use notify_subscription, notify_watch, id \\n9. Collect email address, sid, authenticated \\n10. Use email address, sid, authenticated \\n11. Collect notifications, notify_subscription, notify_watch \\n12. Use notifications, notify_subscription, notify_watch \\n13. Collect notify_subscription, notify_watch, id \\n14. Use notify_subscription, notify_watch, id \\n\\n**Data Types:** \\n1. messages \\n2. username \\n3. notify_subscription \\n4. notify_watch \\n5. id \\n6. email address \\n7. sid \\n8. authenticated \\n9. notifications \\n\\n**Purposes:** \\n1. Functionality \\n2. Customization \\n3. Security \\n4. Functionality \\n5. Customization \\n6. Security \\n\\n**Stories:** \\n1. we (collect messages, username) for (security) We use (use messages, username) for (customization) \\n2. we (collect notify_subscription, notify_watch, id) for (functionality) We use (use notify_subscription, notify_watch, id) for (functionality) \\n3. we (collect notifications, notify_subscription, notify_watch) for (security) We use (use notifications, notify_subscription, notify_watch) for (customization)\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions:\\nCollect, Use, Share\\n\\nR\\nThe text describes actions such as collecting messages, usernames, and notifications (Collect), using notifications for customization and security (Use), and sharing notifications through different channels like email, SMS, or XMPP (Share).\\n\\nData Types:\\nmessages, username, messages, username, sid, authenticated, address, format, realm, category, priority, adverb, klass\\n\\nR\\nThe text references various data types such as messages, usernames, session IDs, addresses, formats, realms, categories, priorities, adverbs, and class names.\\n\\nPurposes:\\nFunctionality, Customization, Security, Tracking, Requirements\\n\\nR\\nThe text describes purposes such as providing a functional notification system (Functionality), allowing customization for users (Customization), ensuring security for users (Security), tracking notifications for various events (Tracking), and meeting requirements for plugin development (Requirements).\\n\\nStories:\\n1. we (collect) messages, username (data type) for (security) purpose. We (use) messages, username (data type) for (customization) purpose.\\n2. we (collect) sid, authenticated, address, format (data types) for (security) purpose. We (use) sid, authenticated, address, format (data types) for (tracking) purpose.\\n3. we (collect) realm, category, priority, adverb (data types) for (functionality) purpose. We (use) realm, category, priority, adverb (data types) for (customization) purpose.\\n4. we (collect) klass (data type) for (functionality) purpose. We (use) klass (data type) for (tracking) purpose.\\n\\nR\\nThese stories represent interactions between actions, data types, and purposes in the context of the text. They highlight how data types are used for various purposes, such as security, customization, tracking, and functionality.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'security']\n",
      "  Matched Labels: {'functionality', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions: Collect, Use, Share\\nData Types: messages, username, messages, username\\nPurposes: Functionality, Customization, Security, Functionality, Customization, Security\\nStories: \\n1. we collect messages, username for security. We use messages, username for customization..\\n\\n1. we collect messages, username for security. We use messages, username for customization.\\nActions: Collect, Use\\nData Types: messages, username\\nPurposes: Security, Customization\\nStories: \\n1. we collect messages, username for security. We use messages, username for customization.\\n\\n1. we collect username for Functionality. We use username for Functionality.\\nActions: Collect, Use\\nData Types: username\\nPurposes: Functionality, Functionality\\nStories: \\n1. we collect username for Functionality. We use username for Functionality.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations:\\n\\nActions:\\nCollect, Use, Share \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'functionality', 'security', 'customization'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Actions: \\nCollect, Use, Share\\nCollect: Collect User ID, Job Title, Company, Address, Name, Date of Birth, Image, Government ID, Biographical Data, CV, Education, Employment, Physical activity, Orders, Payment History, Purchase History, Order, Card Data, Bank Account, Credit Score, Income Information, assets, vehicle, Insurance, App Interactions, Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history, Cookies, Web Beacons, Tags, Messages, Friends\\nUse: Use Contact Data, Health Data, Social Media, Location, Financial, Usage Data, Account Information\\nShare: Share Email address, username\\n\\nData Types: \\nContact Data: Phone Number, Email address, User ID, Job Title, Company, Address, Name, Date of Birth, Image, Government ID, Biographical Data, CV, Education, Employment\\nHealth Data: Physical activity\\nSocial Media: Social Media\\nLocation: Approximate location, Precise location\\nFinancial: Orders, Payment History, Purchase History, Order, Card Data, Bank Account, Credit Score, Income Information, assets\\nUsage Data: App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history\\nTracking: Cookies, Web Beacons, Tags\\nAccount Information: User id, Username, Password, Account Balance, Messages, Friends\\n\\nPurposes: \\nFunctionality, Security, Customization, Advertisement, Accounts, Requirements\\nFunctionality: Collect, Use, Share\\nSecurity: Collect User ID, Job Title, Company, Address, Name, Date of Birth, Image, Government ID, Biographical Data, CV, Education, Employment, Physical activity, Orders, Payment History, Purchase History, Order, Card Data, Bank Account, Credit Score, Income Information, assets, vehicle, Insurance\\nCustomization: Use Contact Data, Health Data, Social Media, Location, Financial, Usage Data, Account Information\\nAdvertisement: Use App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history\\nAccounts: Use User ID, Username, Password, Account Balance, Messages, Friends\\nRequirements: Use Contact Data, Health Data, Social Media, Location, Financial, Usage Data, Account Information\\n\\nStories: \\n1. We use session ID and app interactions  for security. We collect app interactions and session ID for functionality.\\nRationale: The text describes the use of session ID and app interactions for security, and collecting them for functionality. This is an example of a privacy story that highlights the interaction between data types, purposes, and actions.\\n\\n2. We use contact data for customization. We collect contact data for functionality.\\nRationale: The text describes the use of contact data for customization, and collecting it for functionality. This is an example of a privacy story that highlights the interaction between data types, purposes, and actions.\\n\\n3. We share email address for advertisement. We collect email address for functionality.\\nRationale: The text describes the sharing of email address for advertisement, and collecting it for functionality. This is an example of a privacy story that highlights the interaction between data types, purposes, and actions.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: {'tracking', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: {'tracking', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: {'tracking', 'security'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"Here are the annotations for the given text:\\n\\nActions:\\n- Share\\n- Collect\\n- Use\\n- Enable\\n- Disable\\n- Send\\n- Receive\\n- View\\n- Review\\n- Set\\n- Install\\n- uninstall\\n- Update\\n- Register\\n- Deactivate\\n- Restart\\n- Patch\\n\\nData Types:\\n- Contact Data\\n  - Phone Number\\n  - Email address\\n  - User ID\\n  - Job Title\\n  - Company\\n  - Address\\n  - Name\\n  - Date of Birth\\n  - Image\\n  - Government ID\\n  - Biographical Data\\n    - CV\\n    - Education\\n    - Employment\\n- Health Data\\n  - Physical activity\\n- Social Media\\n- Location\\n  - Approximate location\\n  - Precise location\\n- Financial\\n  - Orders\\n  - Payment History\\n  - Purchase History\\n  - Order\\n  - Card Data\\n  - Bank Account\\n  - Credit Score\\n  - Income Information\\n  - assets\\n    - vehicle\\n    - Insurance\\n- Usage Data\\n  - App Interactions\\n    - Pages Visited\\n    - Timestamps\\n    - Interaction with Ads\\n    - User Engagement\\n    - Session ID\\n  - Device Information\\n    - IP Address\\n    - Device ID\\n    - Advertisement ID\\n    - Browser\\n    - Operating System\\n    - Diagnostics\\n    - Sensor Data\\n    - Audio\\n    - Browsing history\\n- Tracking\\n  - Cookies\\n  - Web Beacons\\n  - Tags\\n- Account Information\\n  - User id\\n  - Username\\n  - Password\\n  - Account Balance\\n  - Messages\\n  - Friends\\n\\nPurposes:\\n- Contact\\n- Analytics\\n- Customization\\n- Advertisement\\n- Security\\n- Tracking\\n- Functionality\\n- Accounts\\n- Requirements\\n\\nStories:\\n1. We use health data and address for requirements. We send health data and address for advertisement. We collect health data and address for analytics. We use health data, address, and usage data for customization.\\n2. We share contact data and social media for contact. We collect contact data and social media for analytics. We use contact data and social media for advertisement.\\n3. We collect location data for location. We use location data for tracking. We share location data for advertisement.\\n4. We use financial data for tracking. We collect financial data for analytics. We use financial data for customization.\\n5. We use data types for functionality. We use data types for security. We use data types for tracking.\\n\\nRationale:\\n\\nThe annotations were made based on the context of the given text. The actions were identified based on the verbs used in the text. For example, \\\"Enable\\\", \\\"Disable\\\", \\\"Send\\\", and \\\"Receive\\\" are actions that are performed in the text. The data types were identified based on the types of data mentioned in the text. For example, \\\"Contact Data\\\", \\\"Health Data\\\", and \\\"Financial Data\\\" are data types that are mentioned in the text. The purposes were identified based on the reasons why the actions and data types are being used. For example, \\\"Contact\\\" is a purpose for sharing contact data. The stories were created by combining the actions, data types, and purposes to describe how the actions, data types, and purposes interact in context. For example, the first story describes how health data and address are used for requirements, advertisement, analytics, and customization.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'security', 'tracking', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**Actions:**\\n1. Share\\n2. Collect\\n3. Use\\n4. Enable\\n5. Set\\n6. Send\\n7. Receive\\n8. Disable\\n9. Update\\n10. Register\\n11. Run\\n12. Comment\\n13. Restart\\n14. Patch\\n15. Distribute\\n\\n\n",
      "\n",
      "Annotation F1 Scores:\n",
      "File: Accounts_&_Transactions.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.8571\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8857\n",
      "----------------------------------------\n",
      "File: Accounts_&_Transactions.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.4667\n",
      "----------------------------------------\n",
      "File: CreatingAnewAccount.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: CreatingAnewAccount.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "File: features.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.4444\n",
      "----------------------------------------\n",
      "File: features.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.5556\n",
      "----------------------------------------\n",
      "File: add_threePids.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.8889\n",
      "data_types F1 Score: 0.8333\n",
      "purposes F1 Score: 0.2857\n",
      "Overall F1 Score: 0.6693\n",
      "----------------------------------------\n",
      "File: add_threePids.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.8333\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.9444\n",
      "----------------------------------------\n",
      "File: account-basics.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: account-basics.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: signups.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.9333\n",
      "----------------------------------------\n",
      "File: signups.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.9333\n",
      "----------------------------------------\n",
      "File: userdetails.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: userdetails.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: configurations.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: configurations.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Getting Started.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Getting Started.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.7222\n",
      "----------------------------------------\n",
      "File: profile-preferences.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: profile-preferences.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: Direct_Messaging_README.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8333\n",
      "----------------------------------------\n",
      "File: Direct_Messaging_README.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8333\n",
      "----------------------------------------\n",
      "File: Navigation.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Navigation.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: README.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: README.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: Architecture.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: Architecture.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: data-structure.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.1667\n",
      "----------------------------------------\n",
      "File: data-structure.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2667\n",
      "----------------------------------------\n",
      "File: managing-users.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.5556\n",
      "----------------------------------------\n",
      "File: managing-users.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: analyrics-events.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.1667\n",
      "----------------------------------------\n",
      "File: analyrics-events.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: account_data_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.5556\n",
      "----------------------------------------\n",
      "File: account_data_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.5556\n",
      "----------------------------------------\n",
      "File: account_validity_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: account_validity_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: password_auth_provider_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: password_auth_provider_callbacks.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: DatabaseSchema.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "File: DatabaseSchema.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.7778\n",
      "----------------------------------------\n",
      "File: INotificationDistributor.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: INotificationDistributor.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8000\n",
      "----------------------------------------\n",
      "File: INotificationSubscriber.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "File: INotificationSubscriber.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: file.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 1.0000\n",
      "----------------------------------------\n",
      "File: file.txt\n",
      "Model: groq:llama-3.2-3b-preview\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "Results saved to annotation_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AnnotationEvaluator:\n",
    "    def __init__(self, ontology_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the privacy ontology.\n",
    "        \"\"\"\n",
    "        with open(ontology_path, 'r') as f:\n",
    "            self.ontology = json.load(f)\n",
    "    \n",
    "    def _normalize_label(self, label: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normalize label by removing extra whitespace, converting to lowercase,\n",
    "        and splitting comma-separated values.\n",
    "        \"\"\"\n",
    "        # Split by comma, strip whitespace, convert to lowercase\n",
    "        return [\n",
    "            l.strip().lower() \n",
    "            for l in label.split(',') \n",
    "            if l.strip()\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text by removing content within or after <R> tags.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to preprocess\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        # Find all <R> tags\n",
    "        r_tags = list(re.finditer(r'<R>(.*?)</R>', text, re.DOTALL))\n",
    "        \n",
    "        if not r_tags:\n",
    "            # If no </R> tags, remove everything after a single <R> tag\n",
    "            single_r_match = re.search(r'<R>', text)\n",
    "            if single_r_match:\n",
    "                return text[:single_r_match.start()]\n",
    "            return text\n",
    "        \n",
    "        # Take the content before the first <R> tag\n",
    "        preprocessed = text[:r_tags[0].start()]\n",
    "        \n",
    "        # Return preprocessed text\n",
    "        return preprocessed.strip()\n",
    "    \n",
    "    def _match_labels(self, text: str, category_labels: List[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Find matching labels in the text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to search\n",
    "            category_labels (List[str]): Labels to match\n",
    "        \n",
    "        Returns:\n",
    "            Set[str]: Matched labels\n",
    "        \"\"\"\n",
    "        # Preprocess text to remove R-tag content\n",
    "        preprocessed_text = self._preprocess_text(text)\n",
    "        \n",
    "        # Normalize text\n",
    "        normalized_text = preprocessed_text.lower()\n",
    "        \n",
    "        # Match labels\n",
    "        matched_labels = set()\n",
    "        \n",
    "        for label in category_labels:\n",
    "            # Various matching strategies\n",
    "            # 1. Whole word match\n",
    "            word_pattern = r'\\b' + re.escape(label) + r'\\b'\n",
    "            # 2. Partial match\n",
    "            partial_pattern = re.escape(label)\n",
    "            \n",
    "            if (re.search(word_pattern, normalized_text) or \n",
    "                re.search(partial_pattern, normalized_text)):\n",
    "                matched_labels.add(label)\n",
    "        \n",
    "        return matched_labels\n",
    "    \n",
    "    def calculate_comprehensive_f1_scores(self, ground_truth: Dict, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate F1 scores for all categories.\n",
    "        \n",
    "        Args:\n",
    "            ground_truth (Dict): Ground truth annotations\n",
    "            text (str): Text to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, float]: F1 scores\n",
    "        \"\"\"\n",
    "        f1_scores = {}\n",
    "        categories = ['actions', 'data_types', 'purposes']\n",
    "        \n",
    "        for category in categories:\n",
    "            # Extract labels from ground truth\n",
    "            gt_labels = self._normalize_label(', '.join(ground_truth.get(category, [])))\n",
    "            \n",
    "            # Find matching labels in text\n",
    "            pred_labels = self._match_labels(text, gt_labels)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "            y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "            \n",
    "            try:\n",
    "                # Macro average F1 score\n",
    "                f1 = f1_score(y_true, y_pred, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"F1 Score calculation error for {category}: {e}\")\n",
    "                f1 = 0\n",
    "            \n",
    "            f1_scores[category] = f1\n",
    "        \n",
    "        # Calculate overall F1 score\n",
    "        f1_scores['Overall'] = sum(f1_scores.values()) / len(f1_scores)\n",
    "        \n",
    "        # Diagnostic print\n",
    "        print(f\"\\nCategory Diagnostics:\")\n",
    "        for category in categories:\n",
    "            print(f\"{category.capitalize()}:\")\n",
    "            print(f\"  Ground Truth Labels: {gt_labels}\")\n",
    "            print(f\"  Matched Labels: {pred_labels}\")\n",
    "            print(f\"  F1 Score: {f1_scores[category]:.4f}\")\n",
    "        \n",
    "        # Print preprocessed text for verification\n",
    "        print(\"\\nPreprocessed Text:\")\n",
    "        print(self._preprocess_text(text))\n",
    "        \n",
    "        return f1_scores\n",
    "\n",
    "def process_annotation_csv(csv_path: str, ontology_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process the annotation CSV and calculate F1 scores.\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator\n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    # Read the CSV file\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            try:\n",
    "                # Parse target annotations\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                \n",
    "                # Process both model responses\n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    # Extract full text\n",
    "                    full_text = row[response_col]\n",
    "                    \n",
    "                    # Calculate F1 scores\n",
    "                    f1_scores = evaluator.calculate_comprehensive_f1_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare result dictionary\n",
    "                    result = {\n",
    "                        'File': ground_truth.get('file_name', 'Unknown'),\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': f1_scores\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Paths to your files\n",
    "    csv_path = 'Annotated_Outputs/LLM_AnnotationgrokLLAMA3_2-Shreyash.csv'\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    # Process the CSV and calculate F1 scores\n",
    "    results = process_annotation_csv(csv_path, ontology_path)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nAnnotation F1 Scores:\")\n",
    "    for result in results:\n",
    "        print(f\"File: {result['File']}\")\n",
    "        print(f\"Model: {result['Model']}\")\n",
    "        print(f\"Response: {result['Response']}\")\n",
    "        for category, score in result['Metrics'].items():\n",
    "            print(f\"{category} F1 Score: {score:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to a CSV\n",
    "    if results:\n",
    "        output_keys = ['File', 'Model', 'Response', 'Actions F1', 'Data Types F1', 'Purposes F1', 'Overall F1']\n",
    "        with open('annotation_f1_scores.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=output_keys)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for result in results:\n",
    "                # Prepare row for CSV\n",
    "                csv_row = {\n",
    "                    'File': result['File'],\n",
    "                    'Model': result['Model'],\n",
    "                    'Response': result['Response'],\n",
    "                    'Actions F1': result['Metrics'].get('actions', 0),\n",
    "                    'Data Types F1': result['Metrics'].get('data_types', 0),\n",
    "                    'Purposes F1': result['Metrics'].get('purposes', 0),\n",
    "                    'Overall F1': result['Metrics'].get('Overall', 0)\n",
    "                }\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(\"Results saved to annotation_f1_scores.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: Annotated_Outputs/LLMAnnotation_groqGemma.csv\n",
      "File not found: Annotated_Outputs/LLMAnnotation_groqLlama.csv\n",
      "File not found: Annotated_Outputs/LLMAnnotation_gpt4o-latest.csv\n",
      "Processing file: Annotated_Outputs/LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Combined Metrics Across Files:\n",
      "File\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\t0.8350\t1.0000\t0.7167\t0.6010\t1.0000\t0.4296\t0.6200\t1.0000\t0.4493\t0.6853\t1.0000\t0.5318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "class AnnotationEvaluator:\n",
    "    def __init__(self, ontology_path: str):\n",
    "        with open(ontology_path, 'r') as f:\n",
    "            self.ontology = json.load(f)\n",
    "    \n",
    "    def _normalize_label(self, label: str) -> List[str]:\n",
    "        return [\n",
    "            l.strip().lower() \n",
    "            for l in label.split(',') \n",
    "            if l.strip()\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        r_tags = list(re.finditer(r'<R>(.*?)</R>', text, re.DOTALL))\n",
    "        if not r_tags:\n",
    "            single_r_match = re.search(r'<R>', text)\n",
    "            if single_r_match:\n",
    "                return text[:single_r_match.start()]\n",
    "            return text\n",
    "        preprocessed = text[:r_tags[0].start()]\n",
    "        return preprocessed.strip()\n",
    "    \n",
    "    def _match_labels(self, text: str, category_labels: List[str]) -> Set[str]:\n",
    "        preprocessed_text = self._preprocess_text(text)\n",
    "        normalized_text = preprocessed_text.lower()\n",
    "        matched_labels = set()\n",
    "        for label in category_labels:\n",
    "            word_pattern = r'\\b' + re.escape(label) + r'\\b'\n",
    "            partial_pattern = re.escape(label)\n",
    "            if (re.search(word_pattern, normalized_text) or \n",
    "                re.search(partial_pattern, normalized_text)):\n",
    "                matched_labels.add(label)\n",
    "        return matched_labels\n",
    "    \n",
    "    def calculate_comprehensive_scores(self, ground_truth: Dict, text: str) -> Dict[str, float]:\n",
    "        scores = {}\n",
    "        categories = ['actions', 'data_types', 'purposes']\n",
    "        \n",
    "        for category in categories:\n",
    "            gt_labels = self._normalize_label(', '.join(ground_truth.get(category, [])))\n",
    "            pred_labels = self._match_labels(text, gt_labels)\n",
    "            y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "            y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "            \n",
    "            try:\n",
    "                f1 = f1_score(y_true, y_pred, average='binary')\n",
    "                precision = precision_score(y_true, y_pred, average='binary')\n",
    "                recall = recall_score(y_true, y_pred, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics for {category}: {e}\")\n",
    "                f1, precision, recall = 0, 0, 0\n",
    "            \n",
    "            scores[category] = {\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        \n",
    "        overall_f1 = sum([scores[cat]['f1'] for cat in categories]) / len(categories)\n",
    "        overall_precision = sum([scores[cat]['precision'] for cat in categories]) / len(categories)\n",
    "        overall_recall = sum([scores[cat]['recall'] for cat in categories]) / len(categories)\n",
    "        \n",
    "        scores['Overall'] = {\n",
    "            'f1': overall_f1,\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall\n",
    "        }\n",
    "        \n",
    "        return scores\n",
    "\n",
    "def process_annotation_csv(csv_path: str, evaluator: AnnotationEvaluator):\n",
    "    results = []\n",
    "    combined_y_true = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    combined_y_pred = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    full_text = row[response_col]\n",
    "                    scores = evaluator.calculate_comprehensive_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    result = {\n",
    "                        'File': ground_truth.get('file_name', 'Unknown'),\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': scores\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Aggregate data for overall combined metrics\n",
    "                    for category in ['actions', 'data_types', 'purposes']:\n",
    "                        gt_labels = evaluator._normalize_label(', '.join(ground_truth['metadata'].get(category, [])))\n",
    "                        pred_labels = evaluator._match_labels(full_text, gt_labels)\n",
    "                        combined_y_true[category].extend([1 if label in gt_labels else 0 for label in gt_labels])\n",
    "                        combined_y_pred[category].extend([1 if label in pred_labels else 0 for label in gt_labels])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate combined overall metrics\n",
    "    combined_scores = {}\n",
    "    for category in ['actions', 'data_types', 'purposes']:\n",
    "        try:\n",
    "            combined_scores[category] = {\n",
    "                'f1': f1_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "                'precision': precision_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "                'recall': recall_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating combined metrics for {category}: {e}\")\n",
    "            combined_scores[category] = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    overall_f1 = sum([combined_scores[cat]['f1'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_precision = sum([combined_scores[cat]['precision'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_recall = sum([combined_scores[cat]['recall'] for cat in combined_scores]) / len(combined_scores)\n",
    "    \n",
    "    combined_scores['Overall'] = {\n",
    "        'f1': overall_f1,\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall\n",
    "    }\n",
    "    \n",
    "    return results, combined_scores\n",
    "\n",
    "def main():\n",
    "    # Input multiple CSV paths\n",
    "    csv_paths = ['Annotated_Outputs/LLMAnnotation_groqGemma.csv','Annotated_Outputs/LLMAnnotation_groqLlama.csv','Annotated_Outputs/LLMAnnotation_gpt4o-latest.csv','Annotated_Outputs/LLM_AnnotationgrokLLAMA3_2-Shreyash.csv']\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    all_results = {}\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        csv_path = csv_path.strip()\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            continue\n",
    "        print(f\"Processing file: {csv_path}\")\n",
    "        results, combined_scores = process_annotation_csv(csv_path, evaluator)\n",
    "        all_results[os.path.basename(csv_path)] = combined_scores\n",
    "    \n",
    "    # Display results side by side\n",
    "    print(\"\\nComparison of Combined Metrics Across Files:\")\n",
    "    categories = ['actions', 'data_types', 'purposes', 'Overall']\n",
    "    header = [\"File\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(header))\n",
    "    for file_name, scores in all_results.items():\n",
    "        row = [file_name]\n",
    "        for category in categories:\n",
    "            metrics = scores.get(category, {})\n",
    "            row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: LLMAnnotation_groqGemma.csv\n",
      "File not found: LLMAnnotation_groqLlama.csv\n",
      "File not found: LLMAnnotation_gpt4o-latest.csv\n",
      "Processing file: Annotated_Outputs/LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\n",
      "Checking file: Accounts_&_Transactions.txt\n",
      "Checking file: CreatingAnewAccount.txt\n",
      "Checking file: features.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: add_threePids.txt\n",
      "Checking file: account-basics.txt\n",
      "Checking file: signups.txt\n",
      "Checking file: userdetails.txt\n",
      "Checking file: configurations.txt\n",
      "Checking file: Getting Started.txt\n",
      "Checking file: profile-preferences.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: Direct_Messaging_README.txt\n",
      "Checking file: Navigation.txt\n",
      "Checking file: README.txt\n",
      "Checking file: Architecture.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: data-structure.txt\n",
      "Checking file: managing-users.txt\n",
      "Checking file: analyrics-events.txt\n",
      "Checking file: account_data_callbacks.txt\n",
      "Checking file: account_validity_callbacks.txt\n",
      "Checking file: password_auth_provider_callbacks.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: DatabaseSchema.txt\n",
      "Checking file: INotificationDistributor.txt\n",
      "Checking file: INotificationSubscriber.txt\n",
      "Checking file: file.txt\n",
      "File not found: newllama_responses.csv\n",
      "\n",
      "Comparison of Metrics:\n",
      "\n",
      "Overall Metrics:\n",
      "File\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\t0.8350\t1.0000\t0.7167\t0.6010\t1.0000\t0.4296\t0.6200\t1.0000\t0.4493\t0.6853\t1.0000\t0.5318\n",
      "\n",
      "File Type Metrics:\n",
      "File\tFile Type\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\tCode Specification Documents\t0.7797\t1.0000\t0.6389\t0.5753\t1.0000\t0.4038\t0.5806\t1.0000\t0.4091\t0.6452\t1.0000\t0.4839\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\tUser and Developer Guides\t0.8571\t1.0000\t0.7500\t0.7463\t1.0000\t0.5952\t0.6667\t1.0000\t0.5000\t0.7567\t1.0000\t0.6151\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\tREADME Files\t0.5714\t1.0000\t0.4000\t0.6000\t1.0000\t0.4286\t0.6667\t1.0000\t0.5000\t0.6127\t1.0000\t0.4429\n",
      "LLM_AnnotationgrokLLAMA3_2-Shreyash.csv\tArchitecture and Database Design Documents\t0.9206\t1.0000\t0.8529\t0.4186\t1.0000\t0.2647\t0.5926\t1.0000\t0.4211\t0.6439\t1.0000\t0.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def categorize_file(file_name: str) -> str:\n",
    "    \"\"\"Categorize the file based on its name with more robust matching.\"\"\"\n",
    "    file_name_lower = file_name.lower()\n",
    "\n",
    "    print(f\"Checking file: {file_name}\")  # Debug log\n",
    "\n",
    "    architecture_keywords = [\"inotification\", \"database\", \"data-struct\", \"architecture\"]\n",
    "    code_spec_keywords = [\"password\", \"account\", \"analytics\", \"configuration\", \"threepids\"]\n",
    "    readme_keywords = [\"readme\"]\n",
    "\n",
    "    if any(keyword in file_name_lower for keyword in architecture_keywords):\n",
    "        return \"Architecture and Database Design Documents\"\n",
    "    elif any(keyword in file_name_lower for keyword in code_spec_keywords):\n",
    "        return \"Code Specification Documents\"\n",
    "    elif any(keyword in file_name_lower for keyword in readme_keywords):\n",
    "        return \"README Files\"\n",
    "    else:\n",
    "        return \"User and Developer Guides\"\n",
    "\n",
    "def process_annotation_csv_with_file_types(csv_path: str, evaluator, start_row: int = None, end_row: int = None):\n",
    "    \"\"\"\n",
    "    Process the annotation CSV and add file type categorization.\n",
    "    \n",
    "    Args:\n",
    "    csv_path (str): Path to the CSV file\n",
    "    evaluator (AnnotationEvaluator): Annotation evaluator object\n",
    "    start_row (int, optional): Starting row to process (1-indexed)\n",
    "    end_row (int, optional): Ending row to process (1-indexed)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Results, combined scores, and file type scores\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    file_type_metrics = defaultdict(lambda: {\n",
    "        'actions': {'y_true': [], 'y_pred': []},\n",
    "        'data_types': {'y_true': [], 'y_pred': []},\n",
    "        'purposes': {'y_true': [], 'y_pred': []}\n",
    "    })\n",
    "    combined_y_true = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    combined_y_pred = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Convert to list to enable row index tracking\n",
    "        rows = list(reader)\n",
    "        \n",
    "        # Adjust row selection if not specified\n",
    "        if start_row is None:\n",
    "            start_row = 1\n",
    "        if end_row is None:\n",
    "            end_row = len(rows)\n",
    "        \n",
    "        # Validate row selection\n",
    "        start_row = max(1, start_row)\n",
    "        end_row = min(end_row, len(rows))\n",
    "        \n",
    "        # Adjust for 0-indexing\n",
    "        start_index = start_row - 1\n",
    "        end_index = end_row\n",
    "        \n",
    "        for row_index in range(start_index, end_index):\n",
    "            row = rows[row_index]\n",
    "            try:\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                file_name = ground_truth.get('file_name', 'Unknown')\n",
    "                file_type = categorize_file(file_name)\n",
    "                \n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    full_text = row[response_col]\n",
    "                    scores = evaluator.calculate_comprehensive_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    result = {\n",
    "                        'File': file_name,\n",
    "                        'File Type': file_type,\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': scores\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Aggregate data for file type and overall metrics\n",
    "                    for category in ['actions', 'data_types', 'purposes']:\n",
    "                        gt_labels = evaluator._normalize_label(', '.join(ground_truth['metadata'].get(category, [])))\n",
    "                        pred_labels = evaluator._match_labels(full_text, gt_labels)\n",
    "                        \n",
    "                        # Prepare binary labels\n",
    "                        y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "                        y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "                        \n",
    "                        # Aggregate for file type\n",
    "                        file_type_metrics[file_type][category]['y_true'].extend(y_true)\n",
    "                        file_type_metrics[file_type][category]['y_pred'].extend(y_pred)\n",
    "                        \n",
    "                        # Aggregate for overall metrics\n",
    "                        combined_y_true[category].extend(y_true)\n",
    "                        combined_y_pred[category].extend(y_pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row_index + 1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    combined_scores = _calculate_metrics(combined_y_true, combined_y_pred)\n",
    "    \n",
    "    # Calculate file type metrics\n",
    "    file_type_scores = {}\n",
    "    for file_type, type_data in file_type_metrics.items():\n",
    "        file_type_scores[file_type] = _calculate_metrics(\n",
    "            {cat: type_data[cat]['y_true'] for cat in type_data},\n",
    "            {cat: type_data[cat]['y_pred'] for cat in type_data}\n",
    "        )\n",
    "    \n",
    "    return results, combined_scores, file_type_scores\n",
    "\n",
    "def _calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate metrics for different categories.\"\"\"\n",
    "    combined_scores = {}\n",
    "    for category, true_labels in y_true.items():\n",
    "        try:\n",
    "            combined_scores[category] = {\n",
    "                'f1': f1_score(true_labels, y_pred[category], average='binary'),\n",
    "                'precision': precision_score(true_labels, y_pred[category], average='binary'),\n",
    "                'recall': recall_score(true_labels, y_pred[category], average='binary'),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating combined metrics for {category}: {e}\")\n",
    "            combined_scores[category] = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    # Calculate overall scores\n",
    "    overall_f1 = sum([combined_scores[cat]['f1'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_precision = sum([combined_scores[cat]['precision'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_recall = sum([combined_scores[cat]['recall'] for cat in combined_scores]) / len(combined_scores)\n",
    "    \n",
    "    combined_scores['Overall'] = {\n",
    "        'f1': overall_f1,\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall\n",
    "    }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "def main():\n",
    "    csv_paths = ['LLMAnnotation_groqGemma.csv', 'LLMAnnotation_groqLlama.csv', 'LLMAnnotation_gpt4o-latest.csv','Annotated_Outputs/LLM_AnnotationgrokLLAMA3_2-Shreyash.csv','newllama_responses.csv']\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    all_results = {}\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        csv_path = csv_path.strip()\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            continue\n",
    "        print(f\"Processing file: {csv_path}\")\n",
    "        \n",
    "        # Example of using row selection (rows 20-25)\n",
    "        # You can modify these values or pass them as arguments\n",
    "        start_row = 0  # Optional: specify start row (1-indexed)\n",
    "        end_row = 26    # Optional: specify end row (1-indexed)\n",
    "        \n",
    "        _, combined_scores, file_type_scores = process_annotation_csv_with_file_types(\n",
    "            csv_path, \n",
    "            evaluator, \n",
    "            start_row=start_row,  # Optional\n",
    "            end_row=end_row       # Optional\n",
    "        )\n",
    "        \n",
    "        all_results[os.path.basename(csv_path)] = {\n",
    "            'overall': combined_scores,\n",
    "            'file_types': file_type_scores\n",
    "        }\n",
    "    \n",
    "    # Display results side by side\n",
    "    print(\"\\nComparison of Metrics:\")\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    categories = ['actions', 'data_types', 'purposes', 'Overall']\n",
    "    header = [\"File\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(header))\n",
    "    \n",
    "    for file_name, results in all_results.items():\n",
    "        row = [file_name]\n",
    "        for category in categories:\n",
    "            metrics = results['overall'].get(category, {})\n",
    "            row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "        print(\"\\t\".join(row))\n",
    "    \n",
    "    # Print file type metrics\n",
    "    print(\"\\nFile Type Metrics:\")\n",
    "    file_type_header = [\"File\", \"File Type\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(file_type_header))\n",
    "    \n",
    "    for file_name, results in all_results.items():\n",
    "        for file_type, type_scores in results['file_types'].items():\n",
    "            row = [file_name, file_type]\n",
    "            for category in categories:\n",
    "                metrics = type_scores.get(category, {})\n",
    "                row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "            print(\"\\t\".join(row))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for DPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset saved to: privacy_analysis_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Processes the input CSV and outputs a new CSV formatted for Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to save the formatted output CSV.\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Initialize lists for the new dataset\n",
    "    prompts = []\n",
    "    chosen = []\n",
    "    rejected = []\n",
    "\n",
    "    # Process the data\n",
    "    for _, row in data.iterrows():\n",
    "        prompts.append(row['Prompt'])\n",
    "\n",
    "        if row['Preferred_Response'] == 1:\n",
    "            chosen.append(row['Model Response 1'])\n",
    "            rejected.append(row['Model Response 2'])\n",
    "        elif row['Preferred_Response'] == 2:\n",
    "            chosen.append(row['Model Response 2'])\n",
    "            rejected.append(row['Model Response 1'])\n",
    "\n",
    "    # Create a new DataFrame with the desired format\n",
    "    formatted_data = pd.DataFrame({\n",
    "        'prompt': prompts,\n",
    "        'chosen': chosen,\n",
    "        'rejected': rejected\n",
    "    })\n",
    "\n",
    "    # Save the formatted DataFrame to a new CSV\n",
    "    formatted_data.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Formatted dataset saved to: {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = \"dpo_data_gpt4o_llama.csv\"\n",
    "output_csv_path = \"privacy_analysis_dpo.csv\"\n",
    "process_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create response prefference game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to process and create pairs from CSV files\n",
    "def create_model_comparison(csv_files):\n",
    "    # Read all CSVs into pandas DataFrames\n",
    "    dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "    \n",
    "    # Ensure that all DataFrames have the necessary columns: 'Model', 'Preferred_Response', 'Prompt', and 'Target Annotations'\n",
    "    required_columns = {'Model', 'Preferred_Response', 'Model Response 1', 'Model Response 2', 'Prompt', 'Target Annotations'}\n",
    "    \n",
    "    for df in dataframes:\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            raise ValueError(f\"Each CSV must contain the following columns: {', '.join(required_columns)}\")\n",
    "    \n",
    "    # Create a new DataFrame to hold the model pairs and their preferences\n",
    "    model_pairs = []\n",
    "\n",
    "    # Iterate through all possible pairs of DataFrames (4 files = 6 unique pairs)\n",
    "    for i in range(len(dataframes)):\n",
    "        for j in range(i+1, len(dataframes)):\n",
    "            # Get the models and Preferred_Responses from the i-th and j-th CSV\n",
    "            df1 = dataframes[i]\n",
    "            df2 = dataframes[j]\n",
    "            \n",
    "            # Make sure the lengths of the DataFrames are equal, otherwise adjust\n",
    "            min_len = min(len(df1), len(df2))\n",
    "            df1 = df1.head(min_len)\n",
    "            df2 = df2.head(min_len)\n",
    "            \n",
    "            # Compare model names and create a new row with paired models, preferences, Prompt, and Target Annotations\n",
    "            for k in range(min_len):\n",
    "                model1 = df1.iloc[k]['Model']\n",
    "                model2 = df2.iloc[k]['Model']\n",
    "                pref_choice_1 = df1.iloc[k]['Preferred_Response']\n",
    "                pref_choice_2 = df2.iloc[k]['Preferred_Response']\n",
    "                \n",
    "                # Determine the preferred response based on 'Preferred_Response'\n",
    "                response_1 = df1.iloc[k]['Model Response 1'] if pref_choice_1 == 1 else df1.iloc[k]['Model Response 2']\n",
    "                response_2 = df2.iloc[k]['Model Response 1'] if pref_choice_2 == 1 else df2.iloc[k]['Model Response 2']\n",
    "                \n",
    "                # Retain the Prompt and Target Annotations\n",
    "                prompt = df1.iloc[k]['Prompt']\n",
    "                target_annotation = df1.iloc[k]['Target Annotations']\n",
    "                \n",
    "                # Create a comparison row: model names separated by '?' and their responses side by side, also include Prompt and Target Annotations\n",
    "                comparison_row = {\n",
    "                    'Model Pair': f\"{model1}?{model2}\",\n",
    "                    'Preferred_Response 1': response_1,\n",
    "                    'Preferred_Response 2': response_2,\n",
    "                    'Prompt': prompt,\n",
    "                    'Target Annotations': target_annotation\n",
    "                }\n",
    "                model_pairs.append(comparison_row)\n",
    "    \n",
    "    # Convert the list of model pairs into a DataFrame\n",
    "    comparison_df = pd.DataFrame(model_pairs)\n",
    "    \n",
    "    # Save the new CSV with the comparisons, preserving the structure\n",
    "    comparison_df.to_csv(\"model_comparison.csv\", index=False)\n",
    "\n",
    "# Example usage:\n",
    "csv_files = [\n",
    "    'LLMAnnotation_GroqLlama_shashank.csv',\n",
    "    'LLMAnnotation_GroqGemma_Shashank.csv',\n",
    "    'LLMAnnotation-gpt40-shashank.csv',\n",
    "    'LLM_AnnotationgrokLLAMA3_2-Shreyash.csv'\n",
    "]\n",
    "create_model_comparison(csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ELO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elo ratings have been saved to 'elo_ratings.csv'\n",
      "                          Model   Elo Rating\n",
      "0  groq:llama-3.3-70b-versatile  1144.200270\n",
      "1      openai:gpt-4o-2024-11-20  1010.766813\n",
      "2             groq:gemma2-9b-it   937.671544\n",
      "3     groq:llama-3.2-3b-preview   907.361373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baldw\\AppData\\Local\\Temp\\ipykernel_26396\\2821314984.py:47: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  model_pair = row[2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_elo(output_csv, initial_rating=1000, k=32):\n",
    "    \"\"\"\n",
    "    Calculate Elo ratings based on pairwise model comparisons.\n",
    "    \n",
    "    Parameters:\n",
    "        output_csv (str): Path to the CSV file containing model pair evaluations.\n",
    "        initial_rating (int): Initial Elo rating for all models.\n",
    "        k (int): K-factor for Elo calculation, determines rating sensitivity.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing models and their Elo ratings.\n",
    "    \"\"\"\n",
    "    # Read the annotated output CSV\n",
    "    df = pd.read_csv(output_csv)\n",
    "    \n",
    "    # Initialize Elo ratings dictionary\n",
    "    elo_ratings = {}\n",
    "    \n",
    "    # Define a function to calculate the probability of winning\n",
    "    def win_probability(rating_a, rating_b):\n",
    "        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "    \n",
    "    # Define a function to update Elo ratings\n",
    "    def update_ratings(rating_a, rating_b, result_a):\n",
    "        \"\"\"\n",
    "        Updates Elo ratings based on the match result.\n",
    "        \n",
    "        Parameters:\n",
    "            rating_a (float): Current rating of model A.\n",
    "            rating_b (float): Current rating of model B.\n",
    "            result_a (float): Result of model A (1 for win, 0 for loss).\n",
    "        \n",
    "        Returns:\n",
    "            (float, float): Updated ratings for model A and model B.\n",
    "        \"\"\"\n",
    "        prob_a = win_probability(rating_a, rating_b)\n",
    "        prob_b = 1 - prob_a\n",
    "        new_rating_a = rating_a + k * (result_a - prob_a)\n",
    "        new_rating_b = rating_b + k * ((1 - result_a) - prob_b)\n",
    "        return new_rating_a, new_rating_b\n",
    "    \n",
    "    # Process each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract models and their annotated preference\n",
    "        model_pair = row[2]\n",
    "        preferred_response = row['Preferred_Response']\n",
    "        model1, model2 = model_pair.split('?')\n",
    "        \n",
    "        # Initialize Elo ratings for models if not already present\n",
    "        if model1 not in elo_ratings:\n",
    "            elo_ratings[model1] = initial_rating\n",
    "        if model2 not in elo_ratings:\n",
    "            elo_ratings[model2] = initial_rating\n",
    "        \n",
    "        # Determine match result\n",
    "        result_model1 = 1 if preferred_response == 1 else 0\n",
    "        result_model2 = 1 - result_model1\n",
    "        \n",
    "        # Update Elo ratings\n",
    "        elo_ratings[model1], elo_ratings[model2] = update_ratings(\n",
    "            elo_ratings[model1],\n",
    "            elo_ratings[model2],\n",
    "            result_model1\n",
    "        )\n",
    "    \n",
    "    # Convert Elo ratings dictionary to a DataFrame\n",
    "    elo_df = pd.DataFrame(list(elo_ratings.items()), columns=['Model', 'Elo Rating'])\n",
    "    \n",
    "    # Sort models by Elo rating in descending order\n",
    "    elo_df = elo_df.sort_values(by='Elo Rating', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save Elo ratings to a new CSV\n",
    "    elo_df.to_csv(\"elo_ratings.csv\", index=False)\n",
    "    print(\"Elo ratings have been saved to 'elo_ratings.csv'\")\n",
    "    return elo_df\n",
    "\n",
    "# Example usage:\n",
    "output_csv = \"LLMannotated_model_comparision-Shreyash.csv\"  # The annotated output file\n",
    "elo_df = calculate_elo(output_csv)\n",
    "print(elo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
