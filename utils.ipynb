{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use \\n**Data Types:**  Account Information:\\n    Account Balance,\\n    User id\\n**Purposes:** Account management, Functionality\\n\\n**Stories:** 1. We collect Account Balance, User id for account management. We use Account Balance, User id for functionality. \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['personal analytics', 'functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use \\n**Data Types:** Contact Data: Email address, User ID,  Name\\n**Purposes:** Account management, Functionality, Customization, Security\\n**Stories:** \\n1. We collect Email address, Username, Password for account management and Authentication.\\n\\n \\n\\n**Actions:** Collect, Use\\n**Data Types:** Financial: Bank Account, Bank Account Balance\\n**Purposes:** Functionality,  Accounts\\n**Stories:**\\n1. We collect Bank Account, Bank Account Balance to manage user finances and track account balances.\\n\\n\\n## Rationale: \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8889\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n**Data Types:** Account Data: Account Number, Account Balance, Bank Account, Company, Currency, Address, Phone Number, Name \\n**Purposes:** Functionality, Account Management, Personal Analytics\\n\\n\\n**Stories:**\\n\\n* **Story 1:**  We (Collect) Account Data: Account Number, Account Balance, Bank Account, Company, Currency, Address, Phone Number, Name for Functionality. \\n    \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8889\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use \\n**Data Types:** Account Data: Account Number, Account Balance, Bank Account, Address, Company, Name, Phone Number,  \\n**Purposes:** Functionality, Account Management \\n\\n**Stories:** \\n\\n* **Account Management:** Users can add bank accounts and credit cards to the app. The app collects account number, account balance, bank name, phone number, address, and account name for managing user finances. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:**  Contact Data: Name, Email address, Phone Number, Company; Usage Data: App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Browser, Operating System, Diagnostics; Social Media; Financial: Orders, Payment History, Purchase History, Card Data, Bank Account,  ; Account Information: User id, Username, Password, Account Balance, Messages, Friends\\n\\n**Purposes:** Analytics, Customization, Advertisement, Functionality, Accounts, Security,  Contact \\n\\n**Stories:**\\n\\n1. We collect and use Name, Email address, Phone Number, Company,  Orders, Payment History, Purchase History, Card Data, Bank Account for Contact and Functionality.\\n    \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization', 'tracking'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization', 'tracking'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'customization']\n",
      "  Matched Labels: {'customization', 'tracking'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** \\n  * Contact Data:\\n    * Name:\\n    * Email address:\\n    * Phone Number:\\n    * Address:\\n    * User ID:\\n  * Social Media:\\n  * Usage Data:\\n    * App Interactions:\\n      * Pages Visited:\\n      * Timestamps:\\n      * Interaction with Ads:\\n      * User Engagement:\\n      * Session ID:\\n    * Device Information:\\n      * IP Address:\\n      * Device ID:\\n      * Advertisement ID:\\n      * Browser:\\n      * Operating System:\\n      * Diagnostics:\\n      * Sensor Data:\\n      * Audio:\\n      * Browsing history:\\n  * Account Information:\\n    * User id:\\n    * Username:\\n    * Password:\\n\\n**Purposes:** Contact, Analytics, Customization, Advertisement, Security, Tracking, Functionality, Accounts, Requirements\\n\\n**Stories:**\\n1. We collect and use Name, Email address, Phone Number, and Address for Contact. We collect and use User ID for Accounts.  We collect and use Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, and Browsing history for Analytics, Customization, Advertisement, Security, Tracking, and Functionality.\\n2. We use Username and Password for Accounts. We use User ID for Accounts and Security.\\n\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"##  \\n\\n**Actions: Collect, Use**\\n\\n**Data types:\\n\\n**Purposes: Contact, Analytics, Customization, Advertisement, Security\\n\\n**Stories:\\n**\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'accounts', 'functionality', 'contact', 'account management', 'functionality']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Login with the same as the user\\n\\n  \\\"errcode\\\": \\\"M_THREEPID_IN_\\n\\n```json\\n{\\n  \\\"address\\\": \\\"3361122344\\\"\\n}\\n```\\n\\n```json\\n{\\n  \\\"message\\\": \\\"This is the same request and response\\n\\n```json\\n{\\n  \\\"errcode\\\": \\\"M_THREEPID_IN_\\n\\n```\\n\\n```json\\n{\\n  \\\"address\\\": \\\"336122344\\\"\\n}\\n\\n```\\n\\n```json\\n{\\n  \\\"errcode\\\": \\\"M_THREEPID_IN_\\n\\n```\\n\\n```json\\n{\\n  \\\"account\\\": \\\"336122344\\\"\\n}\\n\\n## Add phone number\\n\\n```json\\n\\n\\n```\\n{\\n  \\\"error\\\": \\\"Error contacting \\n```\\n```json\\n{\\n  \\\"error\\\": \\\"M_THREEPID_IN_\\n\\n```\\n\\n```json\\n{\\n  \\\"errcode\\\": \\\"M_THREEPID_IN_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_THREEPID_IN_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_THRE\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_TH\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n```json\\n{\\n  \\\"error\\\": \\\"\\n\\n```json\\n\\n\\n## Removing an email address\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n\\nThis is the same request.\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```\\n\\n```json\\n\\n\\n\\n\\n**\\n\\n```json\\n\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n```json\\n\\n\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"M_\\n\\n```json\\n{\\n  \\\"error\\\": \\\"\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n\\n**Data Types:** Contact Data: Email address, User ID: Nickname, Account Information: Password, Account Information: User ID,  \\n\\n**Purposes:** Account management, Functionality, Security, Customization,\\n\\n**Stories:**\\n\\n1. **We collect Email address, User ID (Nickname), and Password for account management and security purposes.** \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:**  \\n* Contact Data:\\n    * Email address:\\n    * Username:\\n    * Nickname:\\n* Account Information:\\n    * User id:\\n    * Username:\\n    * Password:\\n    * Account Balance:\\n    * Messages:\\n* Usage Data:\\n    * App Interactions:\\n        * Pages Visited:\\n        \\n**Purposes:**  \\n* Account management:\\n* Functionality:\\n* Authentication:\\n* Customization:\\n* Communication:\\n* Security\\n* Analytics\\n\\n**Stories:**\\n\\n1. **We collect Email address, Username, Nickname, and Password for Account management and Authentication.**  We use Nickname for generating web addresses and identifying your account. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:**  Contact Data: Email address, User ID: Username, \\nAccount Information: User id, Username, Password, Account Balance\\n\\n**Purposes:** Account management, Functionality, Security, Customization\\n\\n**Stories:**\\n\\n1. **We collect Email address, User name, password for Account management and Authentication.** \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection', 'functionality']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** Contact Data: Email address,  User ID,  Name, \\n**Purposes:**  Account management, Contact, Functionality, Security\\n\\n**Stories:** \\n\\n1. We collect Email address, User name, password for Account management. We use Email address, User name, password for Authentication. \\n  \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'account management'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share \\n\\n**Data Types:**\\n* Contact Data: Email address, Phone Number \\n* Account Information: User id, Username, Password, Account Balance, Messages, Friends\\n* Usage Data: App Interactions, Device Information\\n\\n**Purposes:**\\n* Security\\n* Functionality\\n* Accounts\\n* Customization\\n\\n**Stories:**\\n1. **We collect email address, User id, Username, Password for account management and security.** \\n    *Rationale:* The text explicitly states that email address, user id, username, and password are used for account management and security purposes. \\n\\n\\n2. **We use Phone Number for your reference only. **\\n    *Rationale:* The text states that the phone number is used for reference only, implying it is collected but not actively used for other purposes.\\n\\n3. **We use App Interactions and Device Information to provide a customized user experience.** \\n    *Rationale:* While not explicitly stated, inferring that user interactions and device information are used to personalize the user interface and functionality.\\n\\n\\n\\n## Annotations:\\n\\n**Actions:** Collect, Use, Share \\n\\n**Data Types:**\\n* Contact Data: Email address, Contact Data: Phone Number \\n* Account Information: User id, Username, Password, Account Balance, Messages, Friends\\n* Usage Data: App Interactions, Device Information\\n\\n**Purposes:**\\n* Security\\n* Functionality\\n* Accounts\\n* Customization\\n\\n**Stories:**\\n1. **We collect email address, User id, Username, Password for account management and security.** \\n    *Rationale:* The text explicitly states that email address, user id, username, and password are used for account management and security purposes. \\n\\n\\n2. **We use Phone Number for your reference only. **\\n    *Rationale:* The text states that the phone number is used for reference only, implying it is collected but not actively used for other purposes.\\n\\n3. **We use App Interactions and Device Information to provide a customized user experience.** \\n    *Rationale:* While not explicitly stated, inferring that user interactions and device information are used to personalize the user interface and functionality.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'identification'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'identification'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'security', 'identification']\n",
      "  Matched Labels: {'security', 'identification'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** \\n* Contact Data: Email address, Phone Number, Name\\n* Account Information:  User id, Username, Password, Account Balance, Messages, Friends\\n* Usage Data: App Interactions (Pages Visited, Timestamps), Device Information (IP Address, Device ID, Browser, Operating System)\\n* Social Media:\\n\\n**Purposes:**\\n* Contact, Analytics, Customization, Security, Functionality, Accounts, Requirements\\n\\n**Stories:**\\n\\n1. **We collect** Email address, Phone Number, and Name **for** User identification and contact purposes **within** the platform. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality', 'analytics'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality', 'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality', 'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n\\n**Data Types:** \\n  *  Usage Data:\\n      * App Interactions:\\n          * Pages Visited\\n          * Timestamps\\n          * Interaction with Ads\\n          * User Engagement\\n          * Session ID\\n      * Device Information:\\n          * IP Address\\n          * Device ID\\n          * Advertisement ID\\n          * Browser\\n          * Operating System\\n          * Diagnostics\\n          * Sensor Data\\n          * Audio\\n  * Account Information:\\n      * Username\\n      * Password \\n  *  Contact Data:\\n      * Email address \\n\\n**Purposes:** Functionality, Security, Analytics,\\n\\n\\n**Stories:**\\n\\n1. We process usage data for Functionality.\\n   *  We process usage data for Security.\\n     * We process App Interactions: Pages Visited, Timestamps, User Engagement for Functionality. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics', 'functionality']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n**Data Types:**  Usage Data:\\n    App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID\\n    Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history\\n**Purposes:** Functionality, Security\\n**Stories:**\\n\\n1. We collect usage data (App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics, Sensor Data, Audio, Browsing history) for Functionality. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Share, Process\\n\\n**Data Types:**  Contact Data: Email address, Account Information: Username, Displayname, Matrix User, Password,  Usage Data:  \\n\\n**Purposes:** Account management, Functionality, Customization,  Contact\\n\\n**Stories:**\\n\\n1. We collect email address and username for account management. We collect displayname and matrix user for functionality. We process password for account management. We share account information for functionality.   \\n2. We collect email address for contact, functionality. We process email address for contact, functionality. \\n\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'functionality', 'authenticate']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Share, Process\\n**Data Types:**  Account Information: User id, Username, Email address, Password,  Contact Data: Name, Usage Data:  App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Device Information:  IP Address,  Device ID,  Browser, Operating System,\\n**Purposes:**  Account Management, Functionality, Security, Customization, Analytics,   \\n**Stories:**\\n1.  We collect Displayname, Bio, Matrix User, Avatar, and Banner for functionality.\\n\\n2. We process  Email and Password for account management. \\n\\n3. We process Name and User ID for functionality\\n\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality', 'identification'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality', 'identification'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality', 'identification'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n**Data Types:**  Contact Data: Email address, Username, Password; Account Information: User id, Username, Password, Account Balance, Messages, Friends\\n**Purposes:** Account Management, Functionality, Authentication, Security, Customization\\n\\n**Stories:**\\n\\n1. **We collect Email address, Username, Password for Account management and Authentication.** \\n    R:  The text explicitly states these data points are required for account creation and login. These actions are performed for the purposes of managing user accounts and verifying their identity.\\n\\n2. **We use Email address for Password resets and Notifications.** \\n    R: The text states that providing an email address is used for sending password reset links and notifications (if enabled).\\n\\n3. **We use Username for display within the platform.**\\n    R: The text states that the username is unique within an instance and is used for identification. \\n\\n4. **We use Password for Authentication.** \\n    R: The text explicitly states that a password is required for logging in to an account.\\n\\n5. **We use Displayname for customization of user profile.**\\n    R: The text states that a display name can be freely changed and is an alternative to the username.\\n\\n6. **We use Bio for user profile customization.**\\n    R: The text states that a bio allows for a long description of the user and can be formatted with Markdown.\\n\\n7. **We use Avatar for user profile customization.**\\n    R: The text states that an avatar is a profile picture that is shown next to all user posts.\\n\\n8. **We use Banner for user profile customization.**\\n    R: The text states that a banner is a header image for the user's profile page.\\n\\n9. **We use Interface language for user interface customization.**\\n    R: The text states that users can select the language they prefer for the user interface.\\n\\n\\n\\nLet me know if you'd like me to annotate any other sections of the text!\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'identification', 'customization']\n",
      "  Matched Labels: {'customization', 'functionality'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:**\\n\\n* **Contact Data:** Email address, Name:\\n* **Account Information:** Username, Password\\n\\n**Purposes:** Account management, Authentication, Functionality, Customization\\n\\n**Stories:**\\n\\n1. **We collect Email address, Username, and Password for Account management and Authentication.** \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:**\\n* **Use:**  The text describes how to set up and use the Direct Messaging service in LibreEHR.\\n \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['compliance', 'functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** \\n* **Use**\\n\\n**Data Types:**\\n* **Account Information: User id, Username, Password\\n\\n**Purpose:**\\n* **Accounts**\\n\\n**Stories:**\\n1. We use user ids and usernames to manage user accounts. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n**Actions:**\\n**Data Types:**\\n**Purposes:**\\n**Stories:**\\n\\n**Actions:**\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Share, Collect, Use\\n\\n\\n**Data Types:** Contact Data: User ID, Account Information: User ID\\n**Purposes:** Security, Functionality, Analytics\\n\\n\\n\\n**Stories:** \\n\\n1. We share User ID for Functionality. R: Sharing user ID is necessary for user authentication and session management, which is required for application functionality. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"##  Annotations \\n\\n**Actions:** Collect, Use, \\n**Data Types:**  Contact Data: Email address, User id:\\n**Purposes:** Account management, Functionality, Authentication\\n**Stories:** \\n1. We collect Email address, User id,  for account management. We use Email address, User id for authentication. \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['contact', 'customization', 'access']\n",
      "  Matched Labels: {'contact'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n**Data Types:**\\n  Contact Data:\\n    Email address:\\n    Username:\\n    Password:\\n **Purposes:** Account management, Functionality, Authentication\\n\\n**Stories:**\\n\\n1. We collect Email address, Username, Password for Account management. We use Email address, Username, Password for Authentication.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** Contact Data: User ID, Usage Data: App Interactions: Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID, Device Information: IP Address, Device ID, Advertisement ID, Browser, Operating System, Diagnostics,  \\n\\n**Purposes:** Functionality, Security, Analytics,\\n\\n**Stories:**\\n1. We process user engagement data for functionality and analytics purposes.  \\n2. We process IP address data for security and analytics purposes. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'compliance', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Process\\n**Data Types:** Usage Data\\n**Purposes:** Functionality, Security\\n**Stories:**\\n\\n1. We process usage data for Functionality. \\n2. We process usage data for Security.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n\\n**Data Types:** Contact Data (Email address), Usage Data (Session ID), Account Information (Username)\\n\\n**Purposes:** Functionality, Security\\n\\n**Stories:**\\n\\n1. We collect Email address for Functionality.  The system stores email addresses to facilitate user communication and notifications within the platform. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.4000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'health analysis', 'customization', 'functionality', 'health analysis', 'customization']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.5000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect\\n**Data Types:** Contact Data: Email address\\n**Purposes:** Functionality\\n\\n**Stories:** We collect Email addresses for functionality. \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.8000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations: \\n\\n**Actions:** Collect, Use, \\n**Data Types:** Contact Data: Email address, User ID: Username, Account Information: Password,\\n**Purposes:** Account management, Functionality, Authentication\\n**Stories:** \\n1. We collect Email address, Username, and Password for account management and authentication. \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.5000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.8000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['account management', 'user protection']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:**  \\n* Contact Data: Email address, Username\\n* Account Information: User id, Username, Password \\n* Usage Data: Interaction with Ads,  Timestamps, Session ID, Device Information: IP Address, Device ID\\n*  \\n\\n**Purposes:** \\n* Contact, Analytics, Customization, Advertisement, Security, Tracking, Functionality, Accounts \\n\\n**Stories:**\\n\\n1. **We collect Email address, Username, Password for account management and authentication.** \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** Contact Data: Email address,  Account Information: User id, User name,  Usage Data: App Interactions: Pages Visited,  Device Information: IP Address, Device ID,   Account Information: Account Balance, Messages, Friends\\n\\n**Purposes:** Functionality, Accounts, Security, Analytics, Customization\\n\\n**Stories:**\\n\\n1. We Collect Email address, User id, User name, IP Address, Device ID for Account Management. We Use Email address, User id, User name, Account Balance, Messages, Friends, Pages Visited for Functionality. We Process IP Address, Device ID for Security.\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 0.8000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['analytics']\n",
      "  Matched Labels: {'analytics'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Share\\n\\n**Data Types:** \\n  * **Contact Data:** Email address, Name, Job Title, Company\\n  * **Account Information:** User ID, Username, Password, Account Balance\\n  * **Usage Data:** App Interactions (Pages Visited, Timestamps, Interaction with Ads, User Engagement, Session ID), Device Information (IP Address, Device ID), Browsing history\\n  * **Social Media:** \\n\\n**Purposes:** \\n   * Accounts, Security, Functionality, Analytics, Customization\\n\\n**Stories:**\\n\\n1. **We collect email address, Name, Job Title, Company for account management.**  \n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**{#s (a: collect, use) (dt: Account data, Username, password, User ID,  (p: Log in)**\\n### `auth_checkers`\\n\\n_First introduced in Synapse v1.46.0_\\n\\n```python\\nauth_checkers: Dict[Tuple[str, Tuple[str, ...], Callable]\\n```\\n\\nA dict mapping from tuples of a login type identifier (such as `m.login.password`) and a\\ntuple of field names (such as `(\\\"password\\\", \\\"secret_thing\\\")` to authentication checking\\ncallbacks, which should be of the following form:\\n\\n```python\\nasync def check_auth(\\n    user: str,\\n    login_type: str,\\n    login_dict: \\\"synapse of module_api.JsonDict\\\",\\n) -> Optional[\\n    Tuple\\n    [\\n        str, \\n        Optional[Callable[[\\\"synapse User id,\\n        Awaitingable[None]]\\n    ]\\n]\\n```\\n\\nThe login type and field names should be provided by the user in the\\nrequest to the `/login` API. [The Matrix specification](https://matrix.org/docs/spec/client_server/latest#authentication-types)\\ndefines some types, however user defined ones are also allowed.\\n\\nThe callback is passed the `user` field provided by the client (which might not be in\\n`@username:server` form), the login type, and a dictionary of login secrets passed by the client.\\n\\nIf the authentication is successful, the module must return the user's Matrix ID (e.\\ng. `@alice:example.com`) and optionally a callback to be called with the response to the\\n`/login` request. If the module doesn't wish to return a callback, it must return `None` instead.\\n\\nIf the authentication is unsuccessful, the module must return `None`.\\n\\nNote that the user is not automatically registered, the `register_user(..) method of the\\n[module API](writing_a_module.html) can be used to lazily create users.\\n\\nIf multiple modules register an auth checker for the same login type but with different fields, Synapse will refuse to start.\\n\\nIf multiple modules register an auth checker for the same login type with the same fields, then the callbacks will be executed in order, until one returns a Matrix User ID (and optionally a callback). In that case, the return value of that callback will be accepted and subsequent callbacks will not be fired.\\nIf every callback returns `None`, then the authentication fails.\\n\\\\}\\n\\n**Rationale:\\n\\nThis section describes how user logins are handled by the system.\\n**\\n\\n**Purpose:**\\nThe purpose of this section is to define how the system handles logins.\\n\\n**Data Types:**\\n- **Account data:** This section deals with user Account data.\\n- **User ID:** Used to identify user accounts.\\n- **Password:** Used for authentication.\\n- **Username:** Used for identifying users.\\n**Actions:**\\n- **Collect:** The system collects user data during the login process.\\n- **Use:** The system uses the collected data for authentication.\\n\\n\\n\\n**Stories:**\\n- We collect username and password for login.\\n- We use the collected information to authenticate the user.\\n- The user is not automatically registered upon login.\\n- If multiple modules register the same login type with different fields, the system will not start.\\n- If multiple modules register the same login type with the same fields, the system executes callbacks until one returns a user ID, then ignores the rest.\\n- If every callback returns None, the authentication fails.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'account management'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**{#s (a: collect, use) (dt: Account data, User id, Room ID, Account data type, json content  (p: Account Management, Auditing)\\n### `on_account_data_updated`\\n\\n_First introduced in Synapse v1.57.0_\\n\\n```python\\nasync def on_account_data_updated(\\n    user_id: str,\\n    room_id: Optional[str],\\n    account_data_type: str,\\n    content: \\\"synapse.module_api.JsonDict\\\",\\n) -> None:\\n```\\n\\nCalled after user's account data has been updated. The module is given the\\nMatrix ID of the user whose account data is changing, the room ID the data is associated\\nwith, the type associated with the change, as well as the new content. If the account\\ndata is not associated with a specific room, then the room ID is `None`.\\n\\nThis callback is triggered when new account data is added or when the data associated with\\na given type (and optionally room) changes. This includes deletion, since in Matrix,\\ndeleting account data consists of replacing the data associated with a given type\\n(and optionally room) with an empty dictionary (`{}`).\\n\\nNote that this doesn't trigger when changing the tags associated with a room, as these are\\nprocessed separately by Synapse.\\n\\nIf multiple modules implement this callback, Synapse runs them all in order.\\n\\n## Example\\n\\nThe example below is a module that implements the `on_account_data_updated` callback, and\\nsends an event to an audit room when a user changes their account data.\\n\\n```python\\nimport json\\nimport attr\\nfrom typing import Any, Dict, Optional\\n\\nfrom synapse.module_api import JsonDict, ModuleApi\\nfrom synapse.module_api.errors import ConfigError\\n\\n\\n@attr.s(auto_\\n        account_data_type: str,\\n        content: JsonDict,\\n    ) -> None:\\n        content_raw = json.dumps(content)\\n        msg_content = f\\\"{user_id} has changed their account data for type {account_data_type} to: {content_raw}\\n        if room_id is not None:\\n            msg_content += f\\\" (in room {room_id}\\\"\\n\\n        await self.api.create_and_send_event_into_room(\\n            {\\n                \\\"room_\\n\\n    \\n            \\\"room_id\\\": self.config.audit_room,\\n                \\\"sender\\\": self.config.sender,\\n                \\\"type\\\": \\\"m.room.message\\\",\\n                \\\"content\\\": {\\n                    \\\"msgtype\\\": \\\"m.text\\\",\\n                    \\\"body\\\": msg_content\\n                }\\n            }\\n        )\\n```\\n  \\n  \\n  \\n#s (a: collect, use) (dt: User ID, Room ID, Account data type, json\\n  (p: Account Management, Auditing)\\n  Stories:\\n1. We collect user ID, room ID, account data type, and json content for account management and auditing.\\n  We use user ID, room ID, account data type, and json content for account management and auditing. We process user ID, room ID, account data type, and json content for\\n  account management and auditing.  We share user ID, room ID, account data type, and json content for account management and auditing.\\n  \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n{#s (a: collect, use) (dt:  Account Information, Username, Password) (p: Log in)\\n### `auth_checkers`\\n\\n_First introduced in Synapse v1.46.0_\\n\\n```python\\nauth_checkers: Dict[Tuple[str, Tuple[str, ...]], Callable]\\n```\\n\\nA dict mapping from tuples of a login type identifier (such as `m.login.password`) and a\\ntuple of field names (such as `(\\\"password\\\", \\\"secret_thing\\\")`) to authentication checking\\ncallbacks, which should be of the following form:\\n\\n```python\\nasync def check_auth(\\n    user: str,\\n    login_type: str,\\n    login_dict: \\\"synapse.module_api.JsonDict\\\",\\n) -> Optional[\\n    Tuple[\\n        str, \\n        Optional[Callable[[\\\"synapse.module_api.LoginResponse\\\"], Awaitable[None]]]\\n]\\n```\\n\\nThe login type and field names should be provided by the user in the\\nrequest to the `/login` API. [The Matrix specification](https://matrix.org/docs/spec/client_server/latest#authentication-types)\\ndefines some types, however user defined ones are also allowed.\\n\\nThe callback is passed the `user` field provided by the client (which might not be in\\n`@username:server` form), the login type, and a dictionary of login secrets passed by\\nthe client.\\n\\nIf the authentication is successful, the module must return the user's Matrix ID (e.g. \\n`@alice:example.com`) and optionally a callback to be called with the response to the\\n`/login` request. If the module doesn't wish to return a callback, it must return `None`\\ninstead.\\n\\nIf the authentication is unsuccessful, the module must return `None`.\\n\\nNote that the user is not automatically registered, the `register_user(..)` method of\\nthe [module API](writing_a-module.html) can be used to lazily create users.\\n\\nIf multiple modules register an auth checker for the same login type but with different fields, Synapse\\nwill refuse to start.\\n\\nIf multiple modules register an auth checker for the same login type with the same fields,\\nthen the callbacks will be executed in order, until one returns a Matrix User ID (and optionally a callback). In that case, the return value of that callback will be accepted and subsequent callbacks will not be fired. If every callback returns `None`, then the authentication fails.\\n\\\\} \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['security', 'account management', 'functionality']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations\\n\\n**{#s (a: collect, use) (dt: Account data, Username, password ) (p: Log in)**\\n### `check_3pid_auth`\\n\\n_First introduced in Synapse v1.46.0_\\n\\n```python\\nasync def check_3pid_auth(\\n    medium: str, \\n    address: str,\\n    password: str,\\n) -> Optional[\\n    Tuple[\\n        str, \\n        Optional[Callable[[\\\"synapse.module_api.LoginResponse\\\"], Awaitable[None]]]\\n]\\n```\\n\\nCalled when a user attempts to register or log in with a third party identifier,\\nsuch as email. It is passed the medium (eg. `email`), an address (eg. `jdoe@example.com`)\\nand the user's password.\\n\\nIf the authentication is successful, the module must return the user's Matrix ID (e.g. \\n`@alice:example.com`) and optionally a callback to be called with the response to the `/login` request.\\nIf the module doesn't wish to return a callback, it must return `None` instead.\\n\\nIf the authentication is unsuccessful, the module must return `None`.\\n\\nIf multiple modules implement this callback, they will be considered in order. If a\\ncallback returns `None`, Synapse falls through to the next one. The value of the first\\ncallback that does not return `None` will be used. If this happens, Synapse will not call\\nany of the subsequent implementations of this callback.\\n\\\\} \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 0.5000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: {'functionality', 'account management'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:**  Process, Collect, Use\\n**Data Types:** Account information, User ID,  Username, Password\\n**Purposes:**  Security, Account management, Functionality, Log in\\n**Stories:** \\n1. We process account information for security. \\n2. We process account information for functionality.\\n3. We process account information for account management.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.6667\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.5000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'account management']\n",
      "  Matched Labels: set()\n",
      "  F1 Score: 0.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n**Data Types:** Account information (Userid, Username, Password)\\n**Purposes:** Security, Login\\n\\n**Stories:**\\n\\n1. We collect account information for security. We use account information for login. \\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n\\n**Data Types:**  Contact Data: Email address,  Account Information: User id,  Usage Data:  Session ID,  Usage Data: Device Information: IP Address, Usage Data:  App Interactions: Pages Visited,  Usage Data: App Interactions: Timestamps,  Social Media: Location: Approximate location,  Social Media: Location: Precise location\\n\\n**Purposes:** Functionality, Accounts, Analytics, Customization, Advertisement, Security, Tracking\\n\\n**Stories:**\\n\\n1. We collect email address, user id, session id for functionality. We use email address, user id for accounts.\\n2. We collect IP address, pages visited, timestamps for analytics. We use IP address, pages visited, timestamps for tracking.\\n3. We collect approximate location, precise location for customization.\\n\\n\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'authentication', 'functionality', 'authentication']\n",
      "  Matched Labels: {'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n**Data Types:** Contact Data: Email address,  Account Information: User id,  Usage Data: Device Information: IP Address,  Usage Data: App Interactions: Pages Visited,  Account Information: Messages\\n**Purposes:** Functionality, Customization, Security, Analytics, Tracking\\n**Stories:** \\n\\n1. **We collect email address, user id for functionality.** We use email address, user id for customization, security.\\n2. **We collect IP Address for analytics, tracking.** \\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.4000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.8571\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use, Use \\n**Data Types:** Contact Data: Email address, Contact Data: Address,  Account Information: User id, Usage Data:  Device Information: IP Address, Usage Data: Device Information: Device ID \\n**Purposes:** Functionality, Security, Customization \\n**Stories:** \\n\\n1. We collect  Contact Data: Email address, Contact Data: Address, Account Information: User id for Functionality. We use Contact Data: Email address, Contact Data: Address, Account Information: User id for Security, Customization.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'security', 'functionality', 'authentication']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.8571\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n\\n**Actions:** Collect, Use\\n**Data Types:**  Contact Data: Email address,  App Interactions: Pages Visited, Usage Data: Device ID, Device Information: IP Address,  Device Information: Browser, Usage Data: Session ID\\n**Purposes:** Functionality, Customization, Security, Analytics, Tracking\\n**Stories:** \\n1. We collect  Contact Data: Email address for Functionality. We use Contact Data: Email address for Customization. \\n2. We collect  App Interactions: Pages Visited for Analytics. We collect  Usage Data: Device ID,  Device Information: IP Address,  Device Information: Browser, Usage Data: Session ID for Tracking.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'customization', 'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'customization', 'security', 'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'customization', 'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations for INotificationSubscriber Text\\n\\n**Actions:** Collect, Use \\n\\n**Data Types:** Session ID, Email address,  Username, App Interactions,  Transport, Realm, Category, Priority, Adverb\\n\\n**Purposes:** Functionality, Security, Customization\\n\\n**Stories:**\\n\\n1. **We collect session IDs for authentication.** \\n   * **Rationale:** The text mentions that `sid` (session ID) can be used for authentication and is included in the `matches()` method to determine if a user is interested in a notification.\\n\\n2. **We collect email addresses to send notifications.**\\n   * **Rationale:** The `address` field in the `matches()` method is described as an email address used for sending notifications.\\n\\n3. **We use app interactions to determine notification relevance.**\\n   * **Rationale:** The `event` object passed to the `matches()` method contains information about the event, which likely includes app interactions.  The text states that the implementation can use \\\"any means to determine if a user is interested in hearing about a given event.\\\"\\n\\n4. **We use transport, realm, category, priority, and adverb to customize notification delivery.**\\n   * **Rationale:** The `matches()` method includes parameters like `distributor` (transport), `realm`, `category`, `priority`, and `adverb`, which are used to filter and prioritize notifications based on user preferences. \\n\\n\\n\\nLet me know if you have any further questions.\"\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.6667\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['functionality', 'customization', 'security', 'functionality', 'customization', 'security']\n",
      "  Matched Labels: {'security', 'functionality'}\n",
      "  F1 Score: 0.8000\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations for Text 2:\\n\\n**Actions:** Collect, Use\\n**Data Types:** Contact Data: Email address, User ID, Account Information: Username, Session ID, Usage Data: App Interactions: Pages Visited\\n**Purposes:** Functionality, Security, Functionality, Authentication\\n**Stories:**\\n1. We collect session IDs and usernames for authentication. \\n2. We use email addresses for sending notifications.\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"**\\n\\nHere are the annotations. \\n\\n**Actions: Share, Collect, Use\\n\\n**Data Types: Contact Data, Health Data, Usage Data\\n\\n**Purposes: Security, Analytics, Functionality\\n\\n**Stories: \\n\\n1. We collect contact data for functionality and security\\n\\n2. We use contact data for security, analytics, and functionality.\\n\\n3. We share contact data for security and functionality.\\n\\nPlease note that this text is just a sample of a single user account on Zulip\\n\\n**\\n\\n\n",
      "\n",
      "Category Diagnostics:\n",
      "Actions:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 1.0000\n",
      "Data_types:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 1.0000\n",
      "Purposes:\n",
      "  Ground Truth Labels: ['tracking', 'security']\n",
      "  Matched Labels: {'security'}\n",
      "  F1 Score: 0.6667\n",
      "\n",
      "Preprocessed Text:\n",
      "\"## Annotations:\\n**Actions:** Share, Collect, Use\\n\\n**Data Types:** Contact Data, Health Data, Usage Data\\n\\n**Purposes:** Account, Security, Analytics, Customization\\n\\n**Story examples:\\n\\n1. We share health data for security\\n\\n2. We share contact data for compliance with regulations\\n3. We collect health data for compliance with regulations\\n4. We use data for security\\n5. We use contact data for security\\n\\n\\n***\\n\\nI'm\\n\\n**\\n\\nThe text provided did not provide sufficient information to annotate further.\\n\\nI need more information to annotate further\\n\\n**\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nThis text lacks sufficient information to annotate further.\\n</R>\\n\\n\\n\\n**\\n\\nPlease provide me with more information to annotate further.\\n**\\n\\n## Annotate further\\n\\n\\nLet me know if you have more details.\\n\\n\\n\\nI need more information to annotate further.\\n\\n\\n**\\n\\nUse case studies to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n**\\n\\nPlease provide me with more information to annotate further.\\n**\\n\\nPlease provide me with more information to annotate further.\\n\\n## Annotate further\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nI need more information to annotate further.\\n\\n\\n**\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n</p>\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n**\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\nPlease provide me with more information to\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annotate further.\\n\\nPlease provide me with more information to annot\\n\\n\\nPlease provide me with\"\n",
      "\n",
      "Annotation F1 Scores:\n",
      "File: Accounts_&_Transactions.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.7333\n",
      "----------------------------------------\n",
      "File: Accounts_&_Transactions.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: CreatingAnewAccount.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.8889\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.9630\n",
      "----------------------------------------\n",
      "File: CreatingAnewAccount.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.8889\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.9630\n",
      "----------------------------------------\n",
      "File: features.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: features.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.6667\n",
      "----------------------------------------\n",
      "File: add_threePids.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.4333\n",
      "----------------------------------------\n",
      "File: add_threePids.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.4667\n",
      "----------------------------------------\n",
      "File: account-basics.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8333\n",
      "----------------------------------------\n",
      "File: account-basics.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.9333\n",
      "----------------------------------------\n",
      "File: signups.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: signups.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.7111\n",
      "----------------------------------------\n",
      "File: userdetails.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: userdetails.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: configurations.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.9333\n",
      "----------------------------------------\n",
      "File: configurations.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.6111\n",
      "----------------------------------------\n",
      "File: Getting Started.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8667\n",
      "----------------------------------------\n",
      "File: Getting Started.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8667\n",
      "----------------------------------------\n",
      "File: profile-preferences.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: profile-preferences.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: Direct_Messaging_README.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Direct_Messaging_README.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Navigation.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: Navigation.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "File: README.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.6333\n",
      "----------------------------------------\n",
      "File: README.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.6333\n",
      "----------------------------------------\n",
      "File: Architecture.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.6000\n",
      "----------------------------------------\n",
      "File: Architecture.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.4889\n",
      "----------------------------------------\n",
      "File: data-structure.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.3889\n",
      "----------------------------------------\n",
      "File: data-structure.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.4000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.5000\n",
      "Overall F1 Score: 0.3000\n",
      "----------------------------------------\n",
      "File: managing-users.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.8000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.6556\n",
      "----------------------------------------\n",
      "File: managing-users.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.5000\n",
      "data_types F1 Score: 0.8000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.6556\n",
      "----------------------------------------\n",
      "File: analyrics-events.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: analyrics-events.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.8000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: account_data_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.2222\n",
      "----------------------------------------\n",
      "File: account_data_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.5556\n",
      "----------------------------------------\n",
      "File: account_validity_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3333\n",
      "----------------------------------------\n",
      "File: account_validity_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.0000\n",
      "data_types F1 Score: 0.0000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.0000\n",
      "----------------------------------------\n",
      "File: password_auth_provider_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.5000\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8333\n",
      "----------------------------------------\n",
      "File: password_auth_provider_callbacks.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 0.6667\n",
      "data_types F1 Score: 0.5000\n",
      "purposes F1 Score: 0.0000\n",
      "Overall F1 Score: 0.3889\n",
      "----------------------------------------\n",
      "File: DatabaseSchema.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: DatabaseSchema.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: INotificationDistributor.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.4000\n",
      "purposes F1 Score: 0.8571\n",
      "Overall F1 Score: 0.7524\n",
      "----------------------------------------\n",
      "File: INotificationDistributor.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.8571\n",
      "Overall F1 Score: 0.8413\n",
      "----------------------------------------\n",
      "File: INotificationSubscriber.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 1.0000\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: INotificationSubscriber.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 0.6667\n",
      "purposes F1 Score: 0.8000\n",
      "Overall F1 Score: 0.8222\n",
      "----------------------------------------\n",
      "File: file.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 1\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "File: file.txt\n",
      "Model: groq:gemma2-9b-it\n",
      "Response: Model Response 2\n",
      "actions F1 Score: 1.0000\n",
      "data_types F1 Score: 1.0000\n",
      "purposes F1 Score: 0.6667\n",
      "Overall F1 Score: 0.8889\n",
      "----------------------------------------\n",
      "Results saved to annotation_f1_scores.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class AnnotationEvaluator:\n",
    "    def __init__(self, ontology_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the privacy ontology.\n",
    "        \"\"\"\n",
    "        with open(ontology_path, 'r') as f:\n",
    "            self.ontology = json.load(f)\n",
    "    \n",
    "    def _normalize_label(self, label: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normalize label by removing extra whitespace, converting to lowercase,\n",
    "        and splitting comma-separated values.\n",
    "        \"\"\"\n",
    "        # Split by comma, strip whitespace, convert to lowercase\n",
    "        return [\n",
    "            l.strip().lower() \n",
    "            for l in label.split(',') \n",
    "            if l.strip()\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess text by removing content within or after <R> tags.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to preprocess\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        # Find all <R> tags\n",
    "        r_tags = list(re.finditer(r'<R>(.*?)</R>', text, re.DOTALL))\n",
    "        \n",
    "        if not r_tags:\n",
    "            # If no </R> tags, remove everything after a single <R> tag\n",
    "            single_r_match = re.search(r'<R>', text)\n",
    "            if single_r_match:\n",
    "                return text[:single_r_match.start()]\n",
    "            return text\n",
    "        \n",
    "        # Take the content before the first <R> tag\n",
    "        preprocessed = text[:r_tags[0].start()]\n",
    "        \n",
    "        # Return preprocessed text\n",
    "        return preprocessed.strip()\n",
    "    \n",
    "    def _match_labels(self, text: str, category_labels: List[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Find matching labels in the text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to search\n",
    "            category_labels (List[str]): Labels to match\n",
    "        \n",
    "        Returns:\n",
    "            Set[str]: Matched labels\n",
    "        \"\"\"\n",
    "        # Preprocess text to remove R-tag content\n",
    "        preprocessed_text = self._preprocess_text(text)\n",
    "        \n",
    "        # Normalize text\n",
    "        normalized_text = preprocessed_text.lower()\n",
    "        \n",
    "        # Match labels\n",
    "        matched_labels = set()\n",
    "        \n",
    "        for label in category_labels:\n",
    "            # Various matching strategies\n",
    "            # 1. Whole word match\n",
    "            word_pattern = r'\\b' + re.escape(label) + r'\\b'\n",
    "            # 2. Partial match\n",
    "            partial_pattern = re.escape(label)\n",
    "            \n",
    "            if (re.search(word_pattern, normalized_text) or \n",
    "                re.search(partial_pattern, normalized_text)):\n",
    "                matched_labels.add(label)\n",
    "        \n",
    "        return matched_labels\n",
    "    \n",
    "    def calculate_comprehensive_f1_scores(self, ground_truth: Dict, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate F1 scores for all categories.\n",
    "        \n",
    "        Args:\n",
    "            ground_truth (Dict): Ground truth annotations\n",
    "            text (str): Text to evaluate\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, float]: F1 scores\n",
    "        \"\"\"\n",
    "        f1_scores = {}\n",
    "        categories = ['actions', 'data_types', 'purposes']\n",
    "        \n",
    "        for category in categories:\n",
    "            # Extract labels from ground truth\n",
    "            gt_labels = self._normalize_label(', '.join(ground_truth.get(category, [])))\n",
    "            \n",
    "            # Find matching labels in text\n",
    "            pred_labels = self._match_labels(text, gt_labels)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "            y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "            \n",
    "            try:\n",
    "                # Macro average F1 score\n",
    "                f1 = f1_score(y_true, y_pred, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"F1 Score calculation error for {category}: {e}\")\n",
    "                f1 = 0\n",
    "            \n",
    "            f1_scores[category] = f1\n",
    "        \n",
    "        # Calculate overall F1 score\n",
    "        f1_scores['Overall'] = sum(f1_scores.values()) / len(f1_scores)\n",
    "        \n",
    "        # Diagnostic print\n",
    "        print(f\"\\nCategory Diagnostics:\")\n",
    "        for category in categories:\n",
    "            print(f\"{category.capitalize()}:\")\n",
    "            print(f\"  Ground Truth Labels: {gt_labels}\")\n",
    "            print(f\"  Matched Labels: {pred_labels}\")\n",
    "            print(f\"  F1 Score: {f1_scores[category]:.4f}\")\n",
    "        \n",
    "        # Print preprocessed text for verification\n",
    "        print(\"\\nPreprocessed Text:\")\n",
    "        print(self._preprocess_text(text))\n",
    "        \n",
    "        return f1_scores\n",
    "\n",
    "def process_annotation_csv(csv_path: str, ontology_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process the annotation CSV and calculate F1 scores.\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator\n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    # Read the CSV file\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            try:\n",
    "                # Parse target annotations\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                \n",
    "                # Process both model responses\n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    # Extract full text\n",
    "                    full_text = row[response_col]\n",
    "                    \n",
    "                    # Calculate F1 scores\n",
    "                    f1_scores = evaluator.calculate_comprehensive_f1_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare result dictionary\n",
    "                    result = {\n",
    "                        'File': ground_truth.get('file_name', 'Unknown'),\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': f1_scores\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Paths to your files\n",
    "    csv_path = 'LLMAnnotation_groqGemma.csv'\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    # Process the CSV and calculate F1 scores\n",
    "    results = process_annotation_csv(csv_path, ontology_path)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nAnnotation F1 Scores:\")\n",
    "    for result in results:\n",
    "        print(f\"File: {result['File']}\")\n",
    "        print(f\"Model: {result['Model']}\")\n",
    "        print(f\"Response: {result['Response']}\")\n",
    "        for category, score in result['Metrics'].items():\n",
    "            print(f\"{category} F1 Score: {score:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to a CSV\n",
    "    if results:\n",
    "        output_keys = ['File', 'Model', 'Response', 'Actions F1', 'Data Types F1', 'Purposes F1', 'Overall F1']\n",
    "        with open('annotation_f1_scores.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=output_keys)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for result in results:\n",
    "                # Prepare row for CSV\n",
    "                csv_row = {\n",
    "                    'File': result['File'],\n",
    "                    'Model': result['Model'],\n",
    "                    'Response': result['Response'],\n",
    "                    'Actions F1': result['Metrics'].get('actions', 0),\n",
    "                    'Data Types F1': result['Metrics'].get('data_types', 0),\n",
    "                    'Purposes F1': result['Metrics'].get('purposes', 0),\n",
    "                    'Overall F1': result['Metrics'].get('Overall', 0)\n",
    "                }\n",
    "                writer.writerow(csv_row)\n",
    "        \n",
    "        print(\"Results saved to annotation_f1_scores.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LLMAnnotation_groqGemma.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LLMAnnotation_groqLlama.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LLMAnnotation_gpt4o-latest.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Combined Metrics Across Files:\n",
      "File\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLMAnnotation_groqGemma.csv\t0.7818\t1.0000\t0.6418\t0.6667\t1.0000\t0.5000\t0.7143\t1.0000\t0.5556\t0.7209\t1.0000\t0.5658\n",
      "LLMAnnotation_groqLlama.csv\t0.8246\t1.0000\t0.7015\t0.7680\t1.0000\t0.6234\t0.7899\t1.0000\t0.6528\t0.7942\t1.0000\t0.6592\n",
      "LLMAnnotation_gpt4o-latest.csv\t0.8348\t1.0000\t0.7164\t0.6550\t1.0000\t0.4870\t0.7368\t1.0000\t0.5833\t0.7422\t1.0000\t0.5956\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "class AnnotationEvaluator:\n",
    "    def __init__(self, ontology_path: str):\n",
    "        with open(ontology_path, 'r') as f:\n",
    "            self.ontology = json.load(f)\n",
    "    \n",
    "    def _normalize_label(self, label: str) -> List[str]:\n",
    "        return [\n",
    "            l.strip().lower() \n",
    "            for l in label.split(',') \n",
    "            if l.strip()\n",
    "        ]\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        r_tags = list(re.finditer(r'<R>(.*?)</R>', text, re.DOTALL))\n",
    "        if not r_tags:\n",
    "            single_r_match = re.search(r'<R>', text)\n",
    "            if single_r_match:\n",
    "                return text[:single_r_match.start()]\n",
    "            return text\n",
    "        preprocessed = text[:r_tags[0].start()]\n",
    "        return preprocessed.strip()\n",
    "    \n",
    "    def _match_labels(self, text: str, category_labels: List[str]) -> Set[str]:\n",
    "        preprocessed_text = self._preprocess_text(text)\n",
    "        normalized_text = preprocessed_text.lower()\n",
    "        matched_labels = set()\n",
    "        for label in category_labels:\n",
    "            word_pattern = r'\\b' + re.escape(label) + r'\\b'\n",
    "            partial_pattern = re.escape(label)\n",
    "            if (re.search(word_pattern, normalized_text) or \n",
    "                re.search(partial_pattern, normalized_text)):\n",
    "                matched_labels.add(label)\n",
    "        return matched_labels\n",
    "    \n",
    "    def calculate_comprehensive_scores(self, ground_truth: Dict, text: str) -> Dict[str, float]:\n",
    "        scores = {}\n",
    "        categories = ['actions', 'data_types', 'purposes']\n",
    "        \n",
    "        for category in categories:\n",
    "            gt_labels = self._normalize_label(', '.join(ground_truth.get(category, [])))\n",
    "            pred_labels = self._match_labels(text, gt_labels)\n",
    "            y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "            y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "            \n",
    "            try:\n",
    "                f1 = f1_score(y_true, y_pred, average='binary')\n",
    "                precision = precision_score(y_true, y_pred, average='binary')\n",
    "                recall = recall_score(y_true, y_pred, average='binary')\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics for {category}: {e}\")\n",
    "                f1, precision, recall = 0, 0, 0\n",
    "            \n",
    "            scores[category] = {\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall\n",
    "            }\n",
    "        \n",
    "        overall_f1 = sum([scores[cat]['f1'] for cat in categories]) / len(categories)\n",
    "        overall_precision = sum([scores[cat]['precision'] for cat in categories]) / len(categories)\n",
    "        overall_recall = sum([scores[cat]['recall'] for cat in categories]) / len(categories)\n",
    "        \n",
    "        scores['Overall'] = {\n",
    "            'f1': overall_f1,\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall\n",
    "        }\n",
    "        \n",
    "        return scores\n",
    "\n",
    "def process_annotation_csv(csv_path: str, evaluator: AnnotationEvaluator):\n",
    "    results = []\n",
    "    combined_y_true = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    combined_y_pred = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    full_text = row[response_col]\n",
    "                    scores = evaluator.calculate_comprehensive_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    result = {\n",
    "                        'File': ground_truth.get('file_name', 'Unknown'),\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': scores\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Aggregate data for overall combined metrics\n",
    "                    for category in ['actions', 'data_types', 'purposes']:\n",
    "                        gt_labels = evaluator._normalize_label(', '.join(ground_truth['metadata'].get(category, [])))\n",
    "                        pred_labels = evaluator._match_labels(full_text, gt_labels)\n",
    "                        combined_y_true[category].extend([1 if label in gt_labels else 0 for label in gt_labels])\n",
    "                        combined_y_pred[category].extend([1 if label in pred_labels else 0 for label in gt_labels])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate combined overall metrics\n",
    "    combined_scores = {}\n",
    "    for category in ['actions', 'data_types', 'purposes']:\n",
    "        try:\n",
    "            combined_scores[category] = {\n",
    "                'f1': f1_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "                'precision': precision_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "                'recall': recall_score(combined_y_true[category], combined_y_pred[category], average='binary'),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating combined metrics for {category}: {e}\")\n",
    "            combined_scores[category] = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    overall_f1 = sum([combined_scores[cat]['f1'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_precision = sum([combined_scores[cat]['precision'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_recall = sum([combined_scores[cat]['recall'] for cat in combined_scores]) / len(combined_scores)\n",
    "    \n",
    "    combined_scores['Overall'] = {\n",
    "        'f1': overall_f1,\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall\n",
    "    }\n",
    "    \n",
    "    return results, combined_scores\n",
    "\n",
    "def main():\n",
    "    # Input multiple CSV paths\n",
    "    csv_paths = ['LLMAnnotation_groqGemma.csv','LLMAnnotation_groqLlama.csv','LLMAnnotation_gpt4o-latest.csv']\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    all_results = {}\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        csv_path = csv_path.strip()\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            continue\n",
    "        print(f\"Processing file: {csv_path}\")\n",
    "        results, combined_scores = process_annotation_csv(csv_path, evaluator)\n",
    "        all_results[os.path.basename(csv_path)] = combined_scores\n",
    "    \n",
    "    # Display results side by side\n",
    "    print(\"\\nComparison of Combined Metrics Across Files:\")\n",
    "    categories = ['actions', 'data_types', 'purposes', 'Overall']\n",
    "    header = [\"File\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(header))\n",
    "    for file_name, scores in all_results.items():\n",
    "        row = [file_name]\n",
    "        for category in categories:\n",
    "            metrics = scores.get(category, {})\n",
    "            row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare huggingface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: LLMAnnotation_groqGemma.csv\n",
      "Checking file: DatabaseSchema.txt\n",
      "Checking file: INotificationDistributor.txt\n",
      "Checking file: INotificationSubscriber.txt\n",
      "Checking file: file.txt\n",
      "Processing file: LLMAnnotation_groqLlama.csv\n",
      "Checking file: DatabaseSchema.txt\n",
      "Checking file: INotificationDistributor.txt\n",
      "Checking file: INotificationSubscriber.txt\n",
      "Checking file: file.txt\n",
      "Processing file: LLMAnnotation_gpt4o-latest.csv\n",
      "Checking file: DatabaseSchema.txt\n",
      "Checking file: INotificationDistributor.txt\n",
      "Checking file: INotificationSubscriber.txt\n",
      "Checking file: file.txt\n",
      "Processing file: LLMAnnotation_grokLLama3_2.csv\n",
      "Checking file: DatabaseSchema.txt\n",
      "Checking file: INotificationDistributor.txt\n",
      "Checking file: INotificationSubscriber.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file: file.txt\n",
      "Processing file: newllama_responses.csv\n",
      "Checking file: DatabaseSchema.txt\n",
      "Error processing row 21: 'Model Response 2'\n",
      "Checking file: INotificationDistributor.txt\n",
      "Error processing row 22: 'Model Response 2'\n",
      "Checking file: INotificationSubscriber.txt\n",
      "Error processing row 23: 'Model Response 2'\n",
      "Checking file: file.txt\n",
      "Error processing row 24: 'Model Response 2'\n",
      "\n",
      "Comparison of Metrics:\n",
      "\n",
      "Overall Metrics:\n",
      "File\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLMAnnotation_groqGemma.csv\t1.0000\t1.0000\t1.0000\t0.7907\t1.0000\t0.6538\t0.8148\t1.0000\t0.6875\t0.8685\t1.0000\t0.7804\n",
      "LLMAnnotation_groqLlama.csv\t0.9231\t1.0000\t0.8571\t0.8182\t1.0000\t0.6923\t0.7692\t1.0000\t0.6250\t0.8368\t1.0000\t0.7248\n",
      "LLMAnnotation_gpt4o-latest.csv\t1.0000\t1.0000\t1.0000\t0.5556\t1.0000\t0.3846\t0.6939\t1.0000\t0.5312\t0.7498\t1.0000\t0.6386\n",
      "LLMAnnotation_grokLLama3_2.csv\t1.0000\t1.0000\t1.0000\t0.5641\t1.0000\t0.3929\t0.7143\t1.0000\t0.5556\t0.7595\t1.0000\t0.6495\n",
      "newllama_responses.csv\t0.9630\t1.0000\t0.9286\t0.6316\t1.0000\t0.4615\t0.7692\t1.0000\t0.6250\t0.7879\t1.0000\t0.6717\n",
      "\n",
      "File Type Metrics:\n",
      "File\tFile Type\tactions F1\tactions Precision\tactions Recall\tdata_types F1\tdata_types Precision\tdata_types Recall\tpurposes F1\tpurposes Precision\tpurposes Recall\tOverall F1\tOverall Precision\tOverall Recall\n",
      "LLMAnnotation_groqGemma.csv\tArchitecture and Database Design Documents\t1.0000\t1.0000\t1.0000\t0.7692\t1.0000\t0.6250\t0.8333\t1.0000\t0.7143\t0.8675\t1.0000\t0.7798\n",
      "LLMAnnotation_groqGemma.csv\tUser and Developer Guides\t1.0000\t1.0000\t1.0000\t1.0000\t1.0000\t1.0000\t0.6667\t1.0000\t0.5000\t0.8889\t1.0000\t0.8333\n",
      "LLMAnnotation_groqLlama.csv\tArchitecture and Database Design Documents\t1.0000\t1.0000\t1.0000\t0.8571\t1.0000\t0.7500\t0.8333\t1.0000\t0.7143\t0.8968\t1.0000\t0.8214\n",
      "LLMAnnotation_groqLlama.csv\tUser and Developer Guides\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\n",
      "LLMAnnotation_gpt4o-latest.csv\tArchitecture and Database Design Documents\t1.0000\t1.0000\t1.0000\t0.5000\t1.0000\t0.3333\t0.6667\t1.0000\t0.5000\t0.7222\t1.0000\t0.6111\n",
      "LLMAnnotation_gpt4o-latest.csv\tUser and Developer Guides\t1.0000\t1.0000\t1.0000\t1.0000\t1.0000\t1.0000\t0.8571\t1.0000\t0.7500\t0.9524\t1.0000\t0.9167\n",
      "LLMAnnotation_grokLLama3_2.csv\tArchitecture and Database Design Documents\t1.0000\t1.0000\t1.0000\t0.5455\t1.0000\t0.3750\t0.7273\t1.0000\t0.5714\t0.7576\t1.0000\t0.6488\n",
      "LLMAnnotation_grokLLama3_2.csv\tUser and Developer Guides\t1.0000\t1.0000\t1.0000\t0.6667\t1.0000\t0.5000\t0.6667\t1.0000\t0.5000\t0.7778\t1.0000\t0.6667\n",
      "newllama_responses.csv\tArchitecture and Database Design Documents\t1.0000\t1.0000\t1.0000\t0.5882\t1.0000\t0.4167\t0.7826\t1.0000\t0.6429\t0.7903\t1.0000\t0.6865\n",
      "newllama_responses.csv\tUser and Developer Guides\t0.6667\t1.0000\t0.5000\t1.0000\t1.0000\t1.0000\t0.6667\t1.0000\t0.5000\t0.7778\t1.0000\t0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Baldw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def categorize_file(file_name: str) -> str:\n",
    "    \"\"\"Categorize the file based on its name with more robust matching.\"\"\"\n",
    "    file_name_lower = file_name.lower()\n",
    "\n",
    "    print(f\"Checking file: {file_name}\")  # Debug log\n",
    "\n",
    "    architecture_keywords = [\"inotification\", \"database\", \"data-struct\", \"architecture\"]\n",
    "    code_spec_keywords = [\"password\", \"account\", \"analytics\", \"configuration\", \"threepids\"]\n",
    "    readme_keywords = [\"readme\"]\n",
    "\n",
    "    if any(keyword in file_name_lower for keyword in architecture_keywords):\n",
    "        return \"Architecture and Database Design Documents\"\n",
    "    elif any(keyword in file_name_lower for keyword in code_spec_keywords):\n",
    "        return \"Code Specification Documents\"\n",
    "    elif any(keyword in file_name_lower for keyword in readme_keywords):\n",
    "        return \"README Files\"\n",
    "    else:\n",
    "        return \"User and Developer Guides\"\n",
    "\n",
    "def process_annotation_csv_with_file_types(csv_path: str, evaluator, start_row: int = None, end_row: int = None):\n",
    "    \"\"\"\n",
    "    Process the annotation CSV and add file type categorization.\n",
    "    \n",
    "    Args:\n",
    "    csv_path (str): Path to the CSV file\n",
    "    evaluator (AnnotationEvaluator): Annotation evaluator object\n",
    "    start_row (int, optional): Starting row to process (1-indexed)\n",
    "    end_row (int, optional): Ending row to process (1-indexed)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Results, combined scores, and file type scores\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    file_type_metrics = defaultdict(lambda: {\n",
    "        'actions': {'y_true': [], 'y_pred': []},\n",
    "        'data_types': {'y_true': [], 'y_pred': []},\n",
    "        'purposes': {'y_true': [], 'y_pred': []}\n",
    "    })\n",
    "    combined_y_true = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    combined_y_pred = {'actions': [], 'data_types': [], 'purposes': []}\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Convert to list to enable row index tracking\n",
    "        rows = list(reader)\n",
    "        \n",
    "        # Adjust row selection if not specified\n",
    "        if start_row is None:\n",
    "            start_row = 1\n",
    "        if end_row is None:\n",
    "            end_row = len(rows)\n",
    "        \n",
    "        # Validate row selection\n",
    "        start_row = max(1, start_row)\n",
    "        end_row = min(end_row, len(rows))\n",
    "        \n",
    "        # Adjust for 0-indexing\n",
    "        start_index = start_row - 1\n",
    "        end_index = end_row\n",
    "        \n",
    "        for row_index in range(start_index, end_index):\n",
    "            row = rows[row_index]\n",
    "            try:\n",
    "                ground_truth = json.loads(row['Target Annotations'])\n",
    "                file_name = ground_truth.get('file_name', 'Unknown')\n",
    "                file_type = categorize_file(file_name)\n",
    "                \n",
    "                for response_col in ['Model Response 1', 'Model Response 2']:\n",
    "                    full_text = row[response_col]\n",
    "                    scores = evaluator.calculate_comprehensive_scores(\n",
    "                        ground_truth['metadata'], \n",
    "                        full_text\n",
    "                    )\n",
    "                    result = {\n",
    "                        'File': file_name,\n",
    "                        'File Type': file_type,\n",
    "                        'Model': row['Model'],\n",
    "                        'Response': response_col,\n",
    "                        'Metrics': scores\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Aggregate data for file type and overall metrics\n",
    "                    for category in ['actions', 'data_types', 'purposes']:\n",
    "                        gt_labels = evaluator._normalize_label(', '.join(ground_truth['metadata'].get(category, [])))\n",
    "                        pred_labels = evaluator._match_labels(full_text, gt_labels)\n",
    "                        \n",
    "                        # Prepare binary labels\n",
    "                        y_true = [1 if label in gt_labels else 0 for label in gt_labels]\n",
    "                        y_pred = [1 if label in pred_labels else 0 for label in gt_labels]\n",
    "                        \n",
    "                        # Aggregate for file type\n",
    "                        file_type_metrics[file_type][category]['y_true'].extend(y_true)\n",
    "                        file_type_metrics[file_type][category]['y_pred'].extend(y_pred)\n",
    "                        \n",
    "                        # Aggregate for overall metrics\n",
    "                        combined_y_true[category].extend(y_true)\n",
    "                        combined_y_pred[category].extend(y_pred)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row_index + 1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    combined_scores = _calculate_metrics(combined_y_true, combined_y_pred)\n",
    "    \n",
    "    # Calculate file type metrics\n",
    "    file_type_scores = {}\n",
    "    for file_type, type_data in file_type_metrics.items():\n",
    "        file_type_scores[file_type] = _calculate_metrics(\n",
    "            {cat: type_data[cat]['y_true'] for cat in type_data},\n",
    "            {cat: type_data[cat]['y_pred'] for cat in type_data}\n",
    "        )\n",
    "    \n",
    "    return results, combined_scores, file_type_scores\n",
    "\n",
    "def _calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate metrics for different categories.\"\"\"\n",
    "    combined_scores = {}\n",
    "    for category, true_labels in y_true.items():\n",
    "        try:\n",
    "            combined_scores[category] = {\n",
    "                'f1': f1_score(true_labels, y_pred[category], average='binary'),\n",
    "                'precision': precision_score(true_labels, y_pred[category], average='binary'),\n",
    "                'recall': recall_score(true_labels, y_pred[category], average='binary'),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating combined metrics for {category}: {e}\")\n",
    "            combined_scores[category] = {'f1': 0, 'precision': 0, 'recall': 0}\n",
    "    \n",
    "    # Calculate overall scores\n",
    "    overall_f1 = sum([combined_scores[cat]['f1'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_precision = sum([combined_scores[cat]['precision'] for cat in combined_scores]) / len(combined_scores)\n",
    "    overall_recall = sum([combined_scores[cat]['recall'] for cat in combined_scores]) / len(combined_scores)\n",
    "    \n",
    "    combined_scores['Overall'] = {\n",
    "        'f1': overall_f1,\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall\n",
    "    }\n",
    "    \n",
    "    return combined_scores\n",
    "\n",
    "def main():\n",
    "    csv_paths = ['LLMAnnotation_groqGemma.csv', 'LLMAnnotation_groqLlama.csv', 'LLMAnnotation_gpt4o-latest.csv','LLMAnnotation_grokLLama3_2.csv','newllama_responses.csv']\n",
    "    ontology_path = 'privacy_ontology_simple.json'\n",
    "    \n",
    "    evaluator = AnnotationEvaluator(ontology_path)\n",
    "    all_results = {}\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        csv_path = csv_path.strip()\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            continue\n",
    "        print(f\"Processing file: {csv_path}\")\n",
    "        \n",
    "        # Example of using row selection (rows 20-25)\n",
    "        # You can modify these values or pass them as arguments\n",
    "        start_row = 21  # Optional: specify start row (1-indexed)\n",
    "        end_row = 26    # Optional: specify end row (1-indexed)\n",
    "        \n",
    "        _, combined_scores, file_type_scores = process_annotation_csv_with_file_types(\n",
    "            csv_path, \n",
    "            evaluator, \n",
    "            start_row=start_row,  # Optional\n",
    "            end_row=end_row       # Optional\n",
    "        )\n",
    "        \n",
    "        all_results[os.path.basename(csv_path)] = {\n",
    "            'overall': combined_scores,\n",
    "            'file_types': file_type_scores\n",
    "        }\n",
    "    \n",
    "    # Display results side by side\n",
    "    print(\"\\nComparison of Metrics:\")\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    categories = ['actions', 'data_types', 'purposes', 'Overall']\n",
    "    header = [\"File\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(header))\n",
    "    \n",
    "    for file_name, results in all_results.items():\n",
    "        row = [file_name]\n",
    "        for category in categories:\n",
    "            metrics = results['overall'].get(category, {})\n",
    "            row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "        print(\"\\t\".join(row))\n",
    "    \n",
    "    # Print file type metrics\n",
    "    print(\"\\nFile Type Metrics:\")\n",
    "    file_type_header = [\"File\", \"File Type\"] + [f\"{category} {metric}\" for category in categories for metric in [\"F1\", \"Precision\", \"Recall\"]]\n",
    "    print(\"\\t\".join(file_type_header))\n",
    "    \n",
    "    for file_name, results in all_results.items():\n",
    "        for file_type, type_scores in results['file_types'].items():\n",
    "            row = [file_name, file_type]\n",
    "            for category in categories:\n",
    "                metrics = type_scores.get(category, {})\n",
    "                row.extend([f\"{metrics.get(metric, 0):.4f}\" for metric in [\"f1\", \"precision\", \"recall\"]])\n",
    "            print(\"\\t\".join(row))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset saved to: privacy_analysis_dpo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_csv(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Processes the input CSV and outputs a new CSV formatted for Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to save the formatted output CSV.\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    data = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Initialize lists for the new dataset\n",
    "    prompts = []\n",
    "    chosen = []\n",
    "    rejected = []\n",
    "\n",
    "    # Process the data\n",
    "    for _, row in data.iterrows():\n",
    "        prompts.append(row['Prompt'])\n",
    "\n",
    "        if row['Preferred_Response'] == 1:\n",
    "            chosen.append(row['Model Response 1'])\n",
    "            rejected.append(row['Model Response 2'])\n",
    "        elif row['Preferred_Response'] == 2:\n",
    "            chosen.append(row['Model Response 2'])\n",
    "            rejected.append(row['Model Response 1'])\n",
    "\n",
    "    # Create a new DataFrame with the desired format\n",
    "    formatted_data = pd.DataFrame({\n",
    "        'prompt': prompts,\n",
    "        'chosen': chosen,\n",
    "        'rejected': rejected\n",
    "    })\n",
    "\n",
    "    # Save the formatted DataFrame to a new CSV\n",
    "    formatted_data.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Formatted dataset saved to: {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = \"dpo_data_gpt4o_llama.csv\"\n",
    "output_csv_path = \"privacy_analysis_dpo.csv\"\n",
    "process_csv(input_csv_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create response prefference game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to process and create pairs from CSV files\n",
    "def create_model_comparison(csv_files):\n",
    "    # Read all CSVs into pandas DataFrames\n",
    "    dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "    \n",
    "    # Ensure that all DataFrames have the necessary columns: 'Model', 'Preferred_Response', 'Prompt', and 'Target Annotations'\n",
    "    required_columns = {'Model', 'Preferred_Response', 'Model Response 1', 'Model Response 2', 'Prompt', 'Target Annotations'}\n",
    "    \n",
    "    for df in dataframes:\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            raise ValueError(f\"Each CSV must contain the following columns: {', '.join(required_columns)}\")\n",
    "    \n",
    "    # Create a new DataFrame to hold the model pairs and their preferences\n",
    "    model_pairs = []\n",
    "\n",
    "    # Iterate through all possible pairs of DataFrames (4 files = 6 unique pairs)\n",
    "    for i in range(len(dataframes)):\n",
    "        for j in range(i+1, len(dataframes)):\n",
    "            # Get the models and Preferred_Responses from the i-th and j-th CSV\n",
    "            df1 = dataframes[i]\n",
    "            df2 = dataframes[j]\n",
    "            \n",
    "            # Make sure the lengths of the DataFrames are equal, otherwise adjust\n",
    "            min_len = min(len(df1), len(df2))\n",
    "            df1 = df1.head(min_len)\n",
    "            df2 = df2.head(min_len)\n",
    "            \n",
    "            # Compare model names and create a new row with paired models, preferences, Prompt, and Target Annotations\n",
    "            for k in range(min_len):\n",
    "                model1 = df1.iloc[k]['Model']\n",
    "                model2 = df2.iloc[k]['Model']\n",
    "                pref_choice_1 = df1.iloc[k]['Preferred_Response']\n",
    "                pref_choice_2 = df2.iloc[k]['Preferred_Response']\n",
    "                \n",
    "                # Determine the preferred response based on 'Preferred_Response'\n",
    "                response_1 = df1.iloc[k]['Model Response 1'] if pref_choice_1 == 1 else df1.iloc[k]['Model Response 2']\n",
    "                response_2 = df2.iloc[k]['Model Response 1'] if pref_choice_2 == 1 else df2.iloc[k]['Model Response 2']\n",
    "                \n",
    "                # Retain the Prompt and Target Annotations\n",
    "                prompt = df1.iloc[k]['Prompt']\n",
    "                target_annotation = df1.iloc[k]['Target Annotations']\n",
    "                \n",
    "                # Create a comparison row: model names separated by '?' and their responses side by side, also include Prompt and Target Annotations\n",
    "                comparison_row = {\n",
    "                    'Model Pair': f\"{model1}?{model2}\",\n",
    "                    'Preferred_Response 1': response_1,\n",
    "                    'Preferred_Response 2': response_2,\n",
    "                    'Prompt': prompt,\n",
    "                    'Target Annotations': target_annotation\n",
    "                }\n",
    "                model_pairs.append(comparison_row)\n",
    "    \n",
    "    # Convert the list of model pairs into a DataFrame\n",
    "    comparison_df = pd.DataFrame(model_pairs)\n",
    "    \n",
    "    # Save the new CSV with the comparisons, preserving the structure\n",
    "    comparison_df.to_csv(\"model_comparison.csv\", index=False)\n",
    "\n",
    "# Example usage:\n",
    "csv_files = [\n",
    "    'LLMAnnotation_GroqLlama_shashank.csv',\n",
    "    'LLMAnnotation_GroqGemma_Shashank.csv',\n",
    "    'LLMAnnotation-gpt40-shashank.csv',\n",
    "    'LLM_AnnotationgrokLLAMA3_2-Shreyash.csv'\n",
    "]\n",
    "create_model_comparison(csv_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ELO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elo ratings have been saved to 'elo_ratings.csv'\n",
      "                          Model   Elo Rating\n",
      "0  groq:llama-3.3-70b-versatile  1144.200270\n",
      "1      openai:gpt-4o-2024-11-20  1010.766813\n",
      "2             groq:gemma2-9b-it   937.671544\n",
      "3     groq:llama-3.2-3b-preview   907.361373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baldw\\AppData\\Local\\Temp\\ipykernel_26396\\2821314984.py:47: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  model_pair = row[2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_elo(output_csv, initial_rating=1000, k=32):\n",
    "    \"\"\"\n",
    "    Calculate Elo ratings based on pairwise model comparisons.\n",
    "    \n",
    "    Parameters:\n",
    "        output_csv (str): Path to the CSV file containing model pair evaluations.\n",
    "        initial_rating (int): Initial Elo rating for all models.\n",
    "        k (int): K-factor for Elo calculation, determines rating sensitivity.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing models and their Elo ratings.\n",
    "    \"\"\"\n",
    "    # Read the annotated output CSV\n",
    "    df = pd.read_csv(output_csv)\n",
    "    \n",
    "    # Initialize Elo ratings dictionary\n",
    "    elo_ratings = {}\n",
    "    \n",
    "    # Define a function to calculate the probability of winning\n",
    "    def win_probability(rating_a, rating_b):\n",
    "        return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "    \n",
    "    # Define a function to update Elo ratings\n",
    "    def update_ratings(rating_a, rating_b, result_a):\n",
    "        \"\"\"\n",
    "        Updates Elo ratings based on the match result.\n",
    "        \n",
    "        Parameters:\n",
    "            rating_a (float): Current rating of model A.\n",
    "            rating_b (float): Current rating of model B.\n",
    "            result_a (float): Result of model A (1 for win, 0 for loss).\n",
    "        \n",
    "        Returns:\n",
    "            (float, float): Updated ratings for model A and model B.\n",
    "        \"\"\"\n",
    "        prob_a = win_probability(rating_a, rating_b)\n",
    "        prob_b = 1 - prob_a\n",
    "        new_rating_a = rating_a + k * (result_a - prob_a)\n",
    "        new_rating_b = rating_b + k * ((1 - result_a) - prob_b)\n",
    "        return new_rating_a, new_rating_b\n",
    "    \n",
    "    # Process each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract models and their annotated preference\n",
    "        model_pair = row[2]\n",
    "        preferred_response = row['Preferred_Response']\n",
    "        model1, model2 = model_pair.split('?')\n",
    "        \n",
    "        # Initialize Elo ratings for models if not already present\n",
    "        if model1 not in elo_ratings:\n",
    "            elo_ratings[model1] = initial_rating\n",
    "        if model2 not in elo_ratings:\n",
    "            elo_ratings[model2] = initial_rating\n",
    "        \n",
    "        # Determine match result\n",
    "        result_model1 = 1 if preferred_response == 1 else 0\n",
    "        result_model2 = 1 - result_model1\n",
    "        \n",
    "        # Update Elo ratings\n",
    "        elo_ratings[model1], elo_ratings[model2] = update_ratings(\n",
    "            elo_ratings[model1],\n",
    "            elo_ratings[model2],\n",
    "            result_model1\n",
    "        )\n",
    "    \n",
    "    # Convert Elo ratings dictionary to a DataFrame\n",
    "    elo_df = pd.DataFrame(list(elo_ratings.items()), columns=['Model', 'Elo Rating'])\n",
    "    \n",
    "    # Sort models by Elo rating in descending order\n",
    "    elo_df = elo_df.sort_values(by='Elo Rating', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save Elo ratings to a new CSV\n",
    "    elo_df.to_csv(\"elo_ratings.csv\", index=False)\n",
    "    print(\"Elo ratings have been saved to 'elo_ratings.csv'\")\n",
    "    return elo_df\n",
    "\n",
    "# Example usage:\n",
    "output_csv = \"LLMannotated_model_comparision-Shreyash.csv\"  # The annotated output file\n",
    "elo_df = calculate_elo(output_csv)\n",
    "print(elo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
